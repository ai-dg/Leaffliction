# Leaffliction
![Score](https://img.shields.io/badge/Score-125%25-brightgreen)  
**Image classification by disease recognition on leaves**

A PyTorch pipeline for multi-class classification of plant leaf diseases (e.g. Apple and Grape) using a custom CNN, image augmentations, and hand-crafted visual transformations (Hue, Masked, AnalyzeImage, PseudoLandmarks) as input channels.

**RÃ©alisateurs :** [ai-dg](https://github.com/ai-dg) Â· [s-t-e-v](https://github.com/s-t-e-v) Â·

---

## ðŸ“š Table of Contents

- [â–Œ Project Overview](#project-overview)
- [â–Œ Features](#features)
- [â–Œ Getting Started](#getting-started) â€” [Demo Notebook](predict_demo.ipynb)
- [â–Œ Usage](#usage)
- [â–Œ Pipeline Overview](#pipeline-overview)
- [â–Œ Hyperparameters and Data Split](#hyperparameters-and-data-split)
- [â–Œ Performance Results](#performance-results)
- [â–Œ Learning Curves](#learning-curves)
- [â–Œ Project Structure](#project-structure)
- [â–Œ Reproducibility](#reproducibility)
- [â–Œ Troubleshooting](#troubleshooting)
- [â–Œ Limitations & Next Steps](#limitations--next-steps)
- [â–Œ Sources and References](#sources-and-references)

---

## â–Œ Project Overview

Leaffliction implements an end-to-end ML workflow for leaf-disease recognition:

1. **Dataset** â€” Directory-based layout (one folder per class).
2. **Distribution** â€” Analyse and visualise class counts (pie/bar).
3. **Augmentation** â€” Balance classes via Albumentations (rotation, blur, contrast, scaling, illumination, perspective).
4. **Transformation** â€” Build multi-channel inputs from plant-specific transforms (Hue, Masked, AnalyzeImage, PseudoLandmarks) for the CNN.
5. **Training** â€” Stratified train/validation split, optional train-data cap via a **capacity** parameter, Adam optimizer, CrossEntropyLoss.
6. **Prediction** â€” Single image or directory, using a saved model (directory or ZIP bundle) and SHA1 signature.

The model is a **Convolutional Neural Network** (Conv2d blocks + GAP + classifier). Training is relatively heavy due to the CNN and the number of transformed samples.

---

## â–Œ Features

âœ”ï¸ **Custom CNN** â€” PyTorch `ConvolutionalNeuralNetwork` with configurable input channels and number of classes.\
âœ”ï¸ **Stratified split** â€” Train/validation split with optional class balance.\
âœ”ï¸ **Augmentation** â€” Albumentations-based augmentation and class balancing.\
âœ”ï¸ **Transform pipeline** â€” Hue, Masked, AnalyzeImage, PseudoLandmarks (and optional Grayscale, Gaussian, ROI, Pseudo) for visual feature channels.\
âœ”ï¸ **Training metrics** â€” Train/validation accuracy per epoch; final evaluation with best checkpoint.\
âœ”ï¸ **Model export** â€” Saved as `model.pth` + `config.json` + `labels.json`, then packaged into a ZIP with SHA1 in `signature.txt`.\
âœ”ï¸ **CLI** â€” Entry points: `Distribution.py`, `Augmentation.py`, `Transformation.py`, `train.py`, `predict.py`.

---

## â–Œ Getting Started

### â–  Requirements

- Python 3.11+ (see `pyproject.toml`: `>=3.11,<3.12`)
- PyTorch, torchvision, OpenCV, Albumentations, PlantCV, matplotlib, numpy

### â–  Environment (Conda)

```bash
conda activate tf
```

Or use the projectâ€™s venv and install dependencies (e.g. from `pyproject.toml` or `requirements.txt` if aligned).

### â–  Dataset layout

Images must be grouped by class in subdirectories:

```
<dataset_dir>/
  Apple_Black_rot/
    image (1).JPG
    ...
  Apple_healthy/
  Apple_rust/
  Apple_scab/
  Grape_Black_rot/
  Grape_Esca/
  Grape_healthy/
  Grape_spot/
```

### â–  Quick commands

```bash
# Distribution (analyse classes)
python Distribution.py ./leaves/images --mode both --save . --verbose

# Augmentation (single image demo)
python Augmentation.py "<path_to_image>" --output-dir augmented_directory --verbose

# Transformation (single image or directory)
python Transformation.py "<path_to_image>" --only hue mask analyze pseudo --verbose
python Transformation.py --src ./leaves/images --dst ./training_artifacts/transform --verbose

# Training (full pipeline; heavy, uses CNN)
python train.py ./leaves/images --out-dir training_artifacts --out-zip train_output.zip --verbose

# Prediction (single image)
python predict.py --model-path ./worked/model ./test_images/Unit_test1/Apple_Black_rot1.JPG

# Prediction (directory; accuracy summary)
python predict.py --model-path ./worked/model --dir-path ./test_images/100 --verbose
```

**Unit tests (10 images):** The model in `./worked/model` is expected to predict correctly the 10 images in `test_images/Unit_test1` and `test_images/Unit_test2`, e.g.:

```bash
python predict.py --model-path ./worked/model ./test_images/Unit_test1/Apple_Black_rot1.JPG
```

### â–  Demo Notebook

The **[predict_demo.ipynb](predict_demo.ipynb)** Jupyter notebook provides a recruiter-friendly, data-scientist-style demo of the **prediction pipeline only** (no training). It runs on CPU and is suitable for Binder-like environments.

**What it demonstrates:**

- Loading the trained model from `worked/model/` (with a clear message if the model is missing).
- Loading an image from a local sample folder (`test_images/Unit_test1`, `Unit_test2`, or `test_images/100/<class>/`) or using a custom path.
- Preprocessing with the projectâ€™s **TransformationEngine** (Hue, Masked, AnalyzeImage, PseudoLandmarks) and visualising the original image and transformed channels.
- Running inference and displaying the **predicted class**, **confidence**, and **top-k** class probabilities.
- Short explanations of the problem, data assumptions, model architecture, and how metrics are produced in the project (train/valid accuracy, directory accuracy), plus limitations and next steps.

Open the notebook from the repo root so that relative paths (`worked/model`, `test_images/`) resolve correctly. Dependencies are those in `requirements.txt` (including `prompt_toolkit>=3.0.48` for the Jupyter kernel; if the kernel fails with â€œmissing module prompt_toolkit.cursor_shapesâ€, run `pip install 'prompt_toolkit>=3.0.48'`).

### â–  Binder Demo (Lightweight Environment)

Binder is configured to use a **minimal, inference-only** environment so that the image stays **around 1â€“2 GB** (instead of ~8 GB) and builds remain within resource limits. Configuration lives in the **`binder/`** folder at the repository root:

- **`binder/runtime.txt`** â€” fixes the Python version (e.g. 3.11) for the Binder build.
- **`binder/requirements.txt`** â€” installs **CPU-only** PyTorch (via `--extra-index-url https://download.pytorch.org/whl/cpu`) and only the packages needed to run **predict_demo.ipynb**: torch, torchvision, numpy, pillow, matplotlib, opencv-python-headless. **PlantCV, rembg and onnxruntime are not installed** to keep the image small; the notebook detects their absence and uses a **simplified preprocessing fallback** (4Ã— grayscale channel), so predictions on Binder may differ slightly from the full pipeline run locally with the full transformation engine.

The rest of the project (local development, training, full CLI) continues to use the root-level environment (e.g. `requirements.txt` or `pyproject.toml`). The root **environment.yml**, if present, is not modified; Binder uses **only** the files under `binder/`.

---

## â–Œ Usage

### â–  Distribution â€” `Distribution.py`

- **Input:** `dataset_dir` (positional).
- **Options:** `--mode {both,bar,pie}`, `--save <path>`, `--verbose`.
- **Output:** Plots (and optional save) of class distribution.

### â–  Augmentation â€” `Augmentation.py`

- **Input:** `image_path` (positional).
- **Options:** `--output-dir` (default `augmented_directory`), `--verbose`.
- **Output:** Grid of augmented images and files in the output directory.

### â–  Transformation â€” `Transformation.py`

- **Input:** Either `image_path` or `--src` + `--dst` for batch.
- **Options:** `--only grayscale|gaussian|mask|hue|roi|analyze|pseudo`, `--verbose`.
- **Output:** Visualisation and/or transformed images (e.g. `*_<TransformName>.png`) under `--dst`.

### â–  Training â€” `train.py`

- **Input:** `dataset_dir` (positional).
- **Options:**
  - `--out-dir` / `-o` (default `training_artifacts`)
  - `--out-zip` (default `train_output.zip`)
  - `--valid-ratio` (default `0.2`)
  - `--seed` (default `42`)
  - `--learning-rate` (default `0.0314`)
  - `--epochs` (default `70`)
  - `--batch-size` (default `8`)
  - `--verbose`
- **Output:** `out_dir/best_model.pth`, `out_dir/model/` (config, labels, weights), ZIP artifact, `signature.txt` (SHA1), and learning curves (see below).

### â–  Prediction â€” `predict.py`

- **Input:** Either an image path or `--dir-path` for a directory.
- **Model:** `--model-path <dir>` or `--model-zip <file>` (one of the two).
- **Options:** `--top-k`, `--show-transforms`, `--verbose`.
- **Output:** Predicted class (and top-k probs); for directory, an accuracy summary.

---

## â–Œ Pipeline Overview

```
Dataset (dir per class)
    â†’ DatasetScanner.scan()
    â†’ DatasetSplitter.split(valid_ratio, seed, stratified=True)
    â†’ [Train] AugmentationEngine.augment_dataset()  (class balancing)
    â†’ TransformationDirectory.run() on original + augmented
    â†’ TransformationEngine.extract_transformed_items() / load_transformer_items()
         â€¢ Train: capacity=0.5  (see Hyperparameters)
         â€¢ Valid: capacity=1.0
    â†’ DataLoader (shuffle train)
    â†’ ConvolutionalNeuralNetwork + CrossEntropyLoss + Adam
    â†’ Best checkpoint (validation accuracy) â†’ best_model.pth
    â†’ InferenceManager.save() â†’ model/ + ZIP + signature.txt
```

---

## â–Œ Hyperparameters and Data Split

### â–  Train script (`train.py`)

`TrainConfig` is built from CLI (with fallbacks):

| Parameter       | CLI / default in code | Description                    |
|----------------|------------------------|--------------------------------|
| `epochs`       | `--epochs`, default 70 | Number of training epochs.     |
| `batch_size`   | `--batch-size`, default 8 | Batch size.                 |
| `lr`           | `--learning-rate` â†’ 0.0314 or 1e-3 (see code) | Learning rate. |
| `valid_ratio`  | `--valid-ratio`, default 0.2 | Fraction of data for validation. |
| `seed`         | `--seed`, default 42   | Random seed.                   |
| `img_size`     | (224, 224)             | Input spatial size.            |
| `augment_train`| True                   | Use augmentation for training. |
| `transform_train` | True                | Use transform pipeline.       |

### â–  Data capacity (`train_pipeline.py`)

Training uses a **capacity** limit when loading transformed items:

- **Train:** `load_transformer_items(train_items, capacity=0.5)` â€” keeps **50%** of complete samples per class (randomly), so only part of the transformed training set is used. This acts as a **data split / subsample** to control dataset size and training time.
- **Validation:** `capacity=1.0` â€” all complete validation samples are used.

So effectively: after grouping by (class, base stem) and keeping only â€œcompleteâ€ samples (all 4 transforms), 50% of those train groups are sampled per class; validation uses 100%.

---

## â–Œ Performance Results

### â–  Official training run (reference)

- **Dataset:** Directory with 8 classes (Apple/Grape diseases and healthy).
- **Split:** Stratified; validation ratio 20%. After augmentation and transforms: **10â€¯496** train groups, **1â€¯442** validation groups.
- **Train data cap:** `capacity=0.5` â†’ **5â€¯248** training samples (50% of complete train groups per class).
- **Epochs:** 70. **Device:** CUDA. **Parameters:** 422â€¯632.

**Final metrics (best checkpoint):**

| Metric            | Value    |
|-------------------|----------|
| **Train accuracy** | 99.98% |
| **Valid accuracy** | 98.06% |
| **Training time**  | ~717 s  |

**Epoch progression (excerpt):**

- Epoch 1/70 â€” Train Acc: 19.38%, Valid Acc: 28.02%
- Epoch 35/70 â€” Train Acc: 98.00%, Valid Acc: 96.46%
- Epoch 70/70 â€” Train Acc: 98.91%, Valid Acc: 96.88%

Validation accuracy is required to be â‰¥ 90% and validation set size â‰¥ 100 (enforced by `ModelChecker`).

### â–  Prediction on test sets

- **Directory `./test_images/100`** (152 images): accuracy **94.08%** with `--model-path ./worked/model`.
- **Unit tests:** The model in `./worked/model` is expected to predict correctly all **10** images in `test_images/Unit_test1` and `test_images/Unit_test2` (e.g. `Apple_Black_rot1.JPG` in Unit_test1).

---

## â–Œ Learning Curves

Learning curves are generated at the end of training and saved in the project root (`learning curve.jpg`, `learning curve_loss.jpg`).

| | |
|:---:|:---:|
| ![Train/valid accuracy](learning%20curve.jpg) | ![Train loss](learning%20curve_loss.jpg) |

For headless environments (e.g. no Tk), set:

```bash
MPLBACKEND=Agg python train.py ./leaves/images
```

*(Plots can later be moved to something like `assets/curves/` and linked from this README.)*

---

## â–Œ Project Structure

```
Leaffliction/
â”œâ”€â”€ Distribution.py      # Class distribution analysis
â”œâ”€â”€ Augmentation.py      # Single-image augmentation demo
â”œâ”€â”€ Transformation.py   # Single or batch image transforms
â”œâ”€â”€ train.py             # Training entry point
â”œâ”€â”€ predict.py           # Prediction (image or directory)
â”œâ”€â”€ leaffliction/
â”‚   â”œâ”€â”€ cli.py           # Argument parsers
â”‚   â”œâ”€â”€ dataset.py       # DatasetScanner, DatasetSplitter
â”‚   â”œâ”€â”€ augmentations.py # AugmentationEngine, AugmentationSaver
â”‚   â”œâ”€â”€ transformations.py # TransformationEngine, TransformationDirectory
â”‚   â”œâ”€â”€ model.py         # CNN, LabelMapper, InferenceManager
â”‚   â”œâ”€â”€ train_pipeline.py # Trainer, TrainConfig, Metrics, ModelChecker, TrainingPackager
â”‚   â”œâ”€â”€ predict_pipeline.py # Predictor, PredictConfig
â”‚   â”œâ”€â”€ plotting.py     # Plotter (distribution, learning curves, grids)
â”‚   â””â”€â”€ utils.py         # PathManager, Logger, Hasher, ZipPackager
â”œâ”€â”€ style/
â”‚   â””â”€â”€ leaffliction.mplstyle
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ signature.txt        # SHA1 of the built ZIP
â”œâ”€â”€ learning curve.jpg   # Train/valid accuracy curves
â”œâ”€â”€ learning curve_loss.jpg
â”œâ”€â”€ worked/model/        # Pre-trained model (model.pth, config.json, labels.json)
â””â”€â”€ test_images/         # Unit_test1, Unit_test2, 100, etc.
```

---

## â–Œ Reproducibility

- â–  **Seed:** `--seed` (default 42) is used for dataset split and shuffling; the pipeline uses it where applicable (e.g. in `DatasetSplitter` and capacity sampling in `load_transformer_items`).
- â–  **Versions:** Pin Python and dependencies (e.g. via `pyproject.toml` or `requirements.txt`) for reproducible runs.
- â–  **Capacity:** Using the same `capacity=0.5` in `train_pipeline.py` and the same dataset/split produces comparable train size and behaviour.

---

## â–Œ Troubleshooting

- â–  **Jupyter kernel: "No module named 'prompt_toolkit.cursor_shapes'"** â€” The kernel's `prompt_toolkit` may be old or incomplete; use `--force-reinstall` if a simple upgrade fails. In the **same environment as the kernel** (e.g. conda `tf`), run:
  ```bash
  pip install --force-reinstall --no-cache-dir 'prompt_toolkit>=3.0.48'
  ```
  Then restart the kernel (or restart Jupyter). The repoâ€™s `requirements.txt` already pins `prompt_toolkit>=3.0.48`.
- â–  **`albumentations` / `albucore` conflict:** If pip reports *"albumentations 2.0.8 requires albucore==0.0.24, but you have albucore 0.0.36"*, this project uses **albumentationsx** only (not the old `albumentations` package). Run: `pip uninstall albumentations` so only albumentationsx remains; the code still uses `import albumentations`.

---

## â–Œ Limitations & Next Steps

- â–  **Confusion matrix / classification report** are not computed in the scripts; could be added for validation or test.
- â–  **Learning rate:** Ensure the CLI `--learning-rate` is wired to `TrainConfig.lr` in `train.py` if you want to change it from the command line.
- â–  **Headless:** Use `MPLBACKEND=Agg` when running training without a display to avoid Tk errors.
- â–  **Possible extensions:** export metrics to JSON/CSV, add confusion matrix, move learning curves to `assets/curves/`, and add a small demo dataset under version control for quick runs.

---

## â–Œ Sources and References

The **ConvolutionalNeuralNetwork** class in `leaffliction/model.py` and the overall training/inference setup are inspired by standard PyTorch examples and tutorials: Conv2d stacks, ReLU, MaxPool2d, Global Average Pooling (GAP), and a small MLP classifier with dropout. The architecture is:

- â–  **Features:** four blocks of `Conv2d â†’ ReLU â†’ MaxPool2d` (channels 32 â†’ 64 â†’ 128 â†’ 256), then `AdaptiveAvgPool2d(1)` (GAP).
- â–  **Classifier:** `Flatten â†’ Linear(256, 128) â†’ ReLU â†’ Dropout(0.5) â†’ Linear(128, num_classes)`.

**Inspiration links (CNN, training loop, ResNet-style structure):**

- â–  [PyTorch examples â€” MNIST main](https://github.com/pytorch/examples/blob/main/mnist/main.py)
- â–  [PyTorch-CIFAR â€” main](https://github.com/kuangliu/pytorch-cifar/blob/master/main.py)
- â–  [PyTorch-CIFAR â€” ResNet](https://github.com/kuangliu/pytorch-cifar/blob/master/models/resnet.py)
- â–  [PyTorch tutorial â€” CNN](https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/02-intermediate/convolutional_neural_network/main.py)
- â–  [PyTorch tutorial (repo)](https://github.com/yunjey/pytorch-tutorial)
