# üìñ Guide Conceptuel d'Impl√©mentation ‚Äî Leaffliction (ML Traditionnel)

> **Objectif** : Expliquer **litt√©ralement** ce que chaque classe doit faire, **sans code**, pour une approche **Machine Learning traditionnelle** (SVM, Random Forest, KNN).

---

## üìë Table des mati√®res

1. [Vue d'ensemble : ML Traditionnel vs Deep Learning](#vue-densemble)
2. [Pipeline ML Traditionnel](#pipeline-ml-traditionnel)
3. [leaffliction/cli.py ‚Äî Parsers d'arguments](#cli)
4. [leaffliction/utils.py ‚Äî Utilitaires](#utils)
5. [leaffliction/dataset.py ‚Äî Gestion du dataset](#dataset)
6. [leaffliction/plotting.py ‚Äî Visualisations](#plotting)
7. [leaffliction/augmentations.py ‚Äî Augmentations](#augmentations)
8. [leaffliction/transformations.py ‚Äî Extraction de Features](#transformations)
9. [leaffliction/model.py ‚Äî Mod√®le ML](#model)
10. [leaffliction/train_pipeline.py ‚Äî Pipeline d'entra√Ænement](#train-pipeline)
11. [leaffliction/predict_pipeline.py ‚Äî Pipeline de pr√©diction](#predict-pipeline)

---

<a id="vue-densemble"></a>
## 1. Vue d'ensemble : ML Traditionnel vs Deep Learning

### Diff√©rences Fondamentales

| Aspect | Deep Learning (CNN) | ML Traditionnel |
|--------|-------------------|-----------------|
| **Mod√®le** | R√©seau de neurones | SVM, Random Forest, KNN |
| **Features** | Apprises automatiquement | Extraites manuellement |
| **Donn√©es** | Beaucoup (milliers) | Moins (centaines) |
| **Training** | Lent (GPU) | Rapide (CPU) |
| **Interpr√©tabilit√©** | Faible (bo√Æte noire) | √âlev√©e (features explicites) |

### Pourquoi ML Traditionnel ?

**Avantages** :
- ‚úÖ Plus simple √† comprendre
- ‚úÖ Plus rapide √† entra√Æner
- ‚úÖ Moins de donn√©es n√©cessaires
- ‚úÖ Pas besoin de GPU
- ‚úÖ Features interpr√©tables

**Inconv√©nients** :
- ‚ö†Ô∏è Accuracy potentiellement plus faible
- ‚ö†Ô∏è N√©cessite une bonne extraction de features
- ‚ö†Ô∏è Moins flexible

---

<a id="pipeline-ml-traditionnel"></a>
## 2. Pipeline ML Traditionnel

### Sch√©ma Complet

```
(1) Dataset brut
     ‚Üì
(2) Scan + Split train/valid
     ‚Üì
(3) Augmentation du train set (images physiques)
     ‚Üì Cr√©e plus d'images sur disque
     ‚Üì
(4) Extraction de features (train + valid)
     ‚Üì Transformations ‚Üí vecteurs num√©riques
     ‚Üì Exemple: histogrammes, textures, contours
     ‚Üì
(5) Normalisation (StandardScaler)
     ‚Üì Mean=0, Std=1
     ‚Üì
(6) Entra√Ænement mod√®le ML (SVM, Random Forest, KNN)
     ‚Üì
(7) √âvaluation (accuracy > 90%)
     ‚Üì
(8) Sauvegarde (model.pkl, scaler.pkl, labels.json)
     ‚Üì
(9) Packaging (learnings.zip)
```

### Diff√©rence Cl√© avec CNN

**CNN** :
```
Image ‚Üí CNN ‚Üí Pr√©diction
(Le CNN apprend les features automatiquement)
```

**ML Traditionnel** :
```
Image ‚Üí Extraction Features ‚Üí Mod√®le ML ‚Üí Pr√©diction
(On extrait manuellement les features)
```

---

<a id="cli"></a>
## 3. leaffliction/cli.py ‚Äî Parsers d'arguments

**Statut** : ‚úÖ D√©j√† impl√©ment√©, pas de changement n√©cessaire.

Les parsers restent identiques pour les deux approches.

---

<a id="utils"></a>
## 4. leaffliction/utils.py ‚Äî Utilitaires

**Statut** : ‚úÖ D√©j√† impl√©ment√©, pas de changement n√©cessaire.

Les utilitaires (PathManager, Hasher, ZipPackager) sont identiques.

---

<a id="dataset"></a>
## 5. leaffliction/dataset.py ‚Äî Gestion du dataset

### Changements par rapport √† CNN

**√Ä SUPPRIMER** :
- ‚ùå `TFDataConfig` (pas besoin de tf.data)
- ‚ùå `TFDatasetBuilder` (pas besoin de pipeline TensorFlow)

**√Ä GARDER** :
- ‚úÖ `DatasetIndex`
- ‚úÖ `DatasetScanner`
- ‚úÖ `DatasetSplitter`

---

### **Classe : DatasetScanner**

**M√©thode : scan(root)**

**Ce qu'elle doit faire** :

**√âtape 1 : Lister les sous-dossiers**
- Recevoir un chemin vers le dossier racine (Path)
- Lister TOUS les sous-dossiers directs
- Filtrer pour ne garder que les dossiers (pas les fichiers)
- Trier alphab√©tiquement

**√âtape 2 : Extraire les noms de classes**
- Pour chaque dossier, extraire son nom
- Ces noms deviennent `class_names`
- L'ordre d√©termine les `class_id` (0, 1, 2, ...)

**√âtape 3 : Scanner chaque classe**
- Pour chaque dossier (avec son index comme class_id) :
  - Utiliser `PathManager.ft_iter_images()` pour lister les images
  - Compter le nombre d'images
  - Pour chaque image :
    - Cr√©er un tuple `(chemin_image, class_id)`
    - Ajouter √† la liste `items`
  - Stocker le compte dans `counts`

**√âtape 4 : Retourner**
- Cr√©er un `DatasetIndex` avec toutes ces informations
- Retourner cet objet

**Exemple de structure** :
```
root/
  Apple_Black_rot/     ‚Üê class_id = 0
    image (1).JPG
    image (2).JPG
  Apple_healthy/       ‚Üê class_id = 1
    image (1).JPG
```

---

### **Classe : DatasetSplitter**

**Statut** : ‚úÖ D√©j√† impl√©ment√© (split stratifi√©)

Pas de changement n√©cessaire, fonctionne pour les deux approches.

---

<a id="plotting"></a>
## 6. leaffliction/plotting.py ‚Äî Visualisations

**Statut** : ‚úÖ D√©j√† impl√©ment√©, pas de changement n√©cessaire.

Les visualisations sont identiques pour les deux approches.

---

<a id="augmentations"></a>
## 7. leaffliction/augmentations.py ‚Äî Augmentations

### Changements par rapport √† CNN

**√Ä SUPPRIMER** :
- ‚ùå `KerasAugmentationsFactory` (pas de Keras layers)

**√Ä MODIFIER** :
- ‚úÖ `AugmentationEngine.augment_dataset()` - Devient la m√©thode principale
- ‚úÖ Les augmentations travaillent avec NumPy/OpenCV (pas TensorFlow)

---

### **Utilisation dans ML Traditionnel**

**Diff√©rence cl√©** :
- **CNN** : Augmentations √† la vol√©e pendant le training (dans le pipeline tf.data)
- **ML Traditionnel** : Augmentations cr√©ent des images PHYSIQUES sur disque AVANT le training

**Pourquoi** : Les mod√®les ML traditionnels ne peuvent pas faire d'augmentation √† la vol√©e. On doit cr√©er les images augment√©es une fois, puis extraire leurs features.

---

### **Classe : AugmentationEngine**

#### **M√©thode : augment_dataset(train_items, output_dir, augmentations_per_image)**

**Ce qu'elle doit faire** :

**Param√®tres** :
- `train_items` : Liste de tuples `(Path, class_id)`
- `output_dir` : Dossier o√π sauvegarder les images augment√©es
- `augmentations_per_image` : Nombre d'augmentations par image (ex: 3)

---

**√âtape 1 : Cr√©er le dossier de sortie**
- S'assurer que `output_dir` existe
- Cr√©er les sous-dossiers par classe si n√©cessaire

---

**√âtape 2 : Initialiser la liste de retour**
- Cr√©er une liste vide `augmented_items`

---

**√âtape 3 : Pour chaque image du train set**

**Sous-√©tape 3.1 : Garder l'originale**
- Ajouter `(img_path, label)` √† `augmented_items`

**Sous-√©tape 3.2 : Charger l'image**
- Utiliser OpenCV : `cv2.imread(str(img_path))`
- Convertir BGR ‚Üí RGB : `cv2.cvtColor(img, cv2.COLOR_BGR2RGB)`

**Sous-√©tape 3.3 : Cr√©er N versions augment√©es**
- Pour `i` de 0 √† `augmentations_per_image - 1` :
  - Appliquer `apply_random(img, n=2)` (2 augmentations al√©atoires)
  - G√©n√©rer un nom de fichier : `{stem}_aug{i}{ext}`
  - Cr√©er le chemin complet : `output_dir / class_name / filename`
  - Cr√©er les dossiers parents si n√©cessaire
  - Convertir RGB ‚Üí BGR
  - Sauvegarder avec `cv2.imwrite()`
  - Ajouter `(aug_path, label)` √† `augmented_items`

---

**√âtape 4 : Retourner**
- Retourner `augmented_items` (liste √©tendue : originales + augment√©es)

---

**Exemple** :
```
Input:
  train_items = [
    (Path("Apple_healthy/img1.jpg"), 1),  # 1 image
  ]
  augmentations_per_image = 3

Output:
  augmented_items = [
    (Path("Apple_healthy/img1.jpg"), 1),           # Originale
    (Path("augmented/Apple_healthy/img1_aug0.jpg"), 1),  # Aug 1
    (Path("augmented/Apple_healthy/img1_aug1.jpg"), 1),  # Aug 2
    (Path("augmented/Apple_healthy/img1_aug2.jpg"), 1),  # Aug 3
  ]
  # Total: 4 images (1 originale + 3 augment√©es)
```

---

#### **M√©thode : apply_random(img, n)**

**Ce qu'elle doit faire** :

**Param√®tres** :
- `img` : Image NumPy array
- `n` : Nombre d'augmentations √† appliquer (ex: 2)

---

**√âtape 1 : S√©lectionner n augmentations al√©atoires**
- Utiliser `random.sample(self.augs, n)`
- Cela choisit n augmentations diff√©rentes au hasard

**√âtape 2 : Appliquer s√©quentiellement**
- Copier l'image : `result = img.copy()`
- Pour chaque augmentation s√©lectionn√©e :
  - Appliquer : `result = aug.apply(result)`

**√âtape 3 : Retourner**
- Retourner l'image augment√©e

---

### **Les 6 Augmentations**

Toutes travaillent avec **NumPy arrays** et **OpenCV** (pas TensorFlow).

#### **FlipHorizontalAug**

```python
def apply(self, img: np.ndarray) -> np.ndarray:
    """Flip horizontal avec OpenCV"""
    return cv2.flip(img, 1)  # 1 = horizontal
```

#### **FlipVerticalAug**

```python
def apply(self, img: np.ndarray) -> np.ndarray:
    """Flip vertical avec OpenCV"""
    return cv2.flip(img, 0)  # 0 = vertical
```

#### **RotateAug**

```python
def apply(self, img: np.ndarray) -> np.ndarray:
    """Rotation avec OpenCV"""
    h, w = img.shape[:2]
    center = (w // 2, h // 2)
    M = cv2.getRotationMatrix2D(center, self.angle, 1.0)
    return cv2.warpAffine(img, M, (w, h))
```

#### **BrightnessContrastAug**

```python
def apply(self, img: np.ndarray) -> np.ndarray:
    """Ajuste brightness et contrast"""
    img = img.astype(np.float32)
    img = img * (1 + self.contrast) + self.brightness
    img = np.clip(img, 0, 255)
    return img.astype(np.uint8)
```

#### **GaussianBlurAug**

```python
def apply(self, img: np.ndarray) -> np.ndarray:
    """Gaussian blur avec OpenCV"""
    ksize = int(2 * np.ceil(3 * self.sigma) + 1)
    if ksize % 2 == 0:
        ksize += 1
    return cv2.GaussianBlur(img, (ksize, ksize), self.sigma)
```

#### **RandomCropResizeAug**

```python
def apply(self, img: np.ndarray) -> np.ndarray:
    """Random crop puis resize"""
    h, w = img.shape[:2]
    new_h = int(h * self.crop_ratio)
    new_w = int(w * self.crop_ratio)
    
    top = random.randint(0, h - new_h)
    left = random.randint(0, w - new_w)
    
    cropped = img[top:top+new_h, left:left+new_w]
    return cv2.resize(cropped, (w, h))
```

---

<a id="transformations"></a>
## 8. leaffliction/transformations.py ‚Äî Extraction de Features

### R√¥le dans ML Traditionnel

**Transformations** = **Extraction de Features**

Les transformations ne sont plus juste pour la visualisation, elles sont **essentielles** pour extraire des caract√©ristiques num√©riques des images.

---

### **Classe : FeatureExtractor**

**Responsabilit√©** : Extraire un vecteur de features num√©riques depuis une image.

---

#### **M√©thode : extract_features(img_path)**

**Ce qu'elle doit faire** :

**Param√®tres** :
- `img_path` : Path vers une image

**Retour** :
- `np.ndarray` de shape `(n_features,)` - Vecteur de features

---

**√âtape 1 : Charger l'image**
- Utiliser OpenCV : `cv2.imread(str(img_path))`
- Convertir BGR ‚Üí RGB

---

**√âtape 2 : Extraire features couleur**

**2.1 : Histogramme RGB**
- Pour chaque canal (R, G, B) :
  - Calculer l'histogramme : `np.histogram(img[:,:,channel], bins=256, range=(0, 256))`
  - Normaliser : `hist = hist / hist.sum()`
  - Ajouter les 256 valeurs √† la liste de features
- Total : 256 √ó 3 = 768 features

**2.2 : Statistiques RGB**
- Pour chaque canal :
  - Mean : `img[:,:,channel].mean()`
  - Std : `img[:,:,channel].std()`
  - Min : `img[:,:,channel].min()`
  - Max : `img[:,:,channel].max()`
- Total : 4 √ó 3 = 12 features

---

**√âtape 3 : Appliquer les transformations et extraire stats**

- Pour chaque transformation dans `self.transformations` :
  - Appliquer la transformation : `transformed = tf.apply(img)`
  - Extraire statistiques :
    - Mean
    - Std
    - Min
    - Max
  - Ajouter √† la liste de features
- Total : 4 stats √ó 6 transformations = 24 features

---

**√âtape 4 : (Optionnel) Features de texture**

**Haralick Features** (avec mahotas ou skimage) :
- Convertir en grayscale
- Calculer la matrice de co-occurrence (GLCM)
- Extraire 13 features de Haralick
- Total : 13 features

---

**√âtape 5 : (Optionnel) Features de forme**

**Moments de Hu** :
- Convertir en grayscale
- Binariser (threshold)
- Calculer les moments
- Total : 7 features

---

**√âtape 6 : Concat√©ner et retourner**
- Concat√©ner toutes les features en un seul vecteur
- Convertir en `np.ndarray` de type `float32`
- Retourner

**Total de features** : ~800-1000 features

---

#### **M√©thode : extract_batch(items)**

**Ce qu'elle doit faire** :

**Param√®tres** :
- `items` : Liste de tuples `(Path, class_id)`

**Retour** :
- `X` : np.ndarray de shape `(n_samples, n_features)`
- `y` : np.ndarray de shape `(n_samples,)`

---

**√âtape 1 : Initialiser les listes**
- Cr√©er `X = []` et `y = []`

**√âtape 2 : Pour chaque item**
- Extraire les features : `features = self.extract_features(img_path)`
- Ajouter √† X : `X.append(features)`
- Ajouter le label √† y : `y.append(label)`

**√âtape 3 : Convertir en arrays**
- `X = np.array(X)` ‚Üí shape `(n_samples, n_features)`
- `y = np.array(y)` ‚Üí shape `(n_samples,)`

**√âtape 4 : Retourner**
- Retourner `(X, y)`

---

### **Les 6 Transformations**

Identiques √† la version CNN, mais travaillent avec NumPy/OpenCV.

---

<a id="model"></a>
## 9. leaffliction/model.py ‚Äî Mod√®le ML

### Changements par rapport √† CNN

**√Ä SUPPRIMER** :
- ‚ùå Tout ce qui concerne Keras/TensorFlow
- ‚ùå `ModelFactory` qui construit un CNN

**√Ä AJOUTER** :
- ‚úÖ `MLModelFactory` qui construit un mod√®le sklearn
- ‚úÖ `MLModelBundle` qui sauvegarde avec joblib

---

### **Classe : MLModelFactory**

**Responsabilit√©** : Construire un mod√®le ML traditionnel (sklearn).

---

#### **M√©thode : build(cfg, model_type)**

**Ce qu'elle doit faire** :

**Param√®tres** :
- `cfg` : ModelConfig
- `model_type` : String ("svm", "random_forest", "knn")

**Retour** :
- Mod√®le sklearn non entra√Æn√©

---

**Si model_type == "svm"** :
```python
from sklearn.svm import SVC

return SVC(
    kernel='rbf',        # Radial Basis Function
    C=1.0,               # R√©gularisation
    gamma='scale',       # Coefficient du kernel
    probability=True,    # Pour avoir des probabilit√©s
    random_state=cfg.seed
)
```

**Si model_type == "random_forest"** :
```python
from sklearn.ensemble import RandomForestClassifier

return RandomForestClassifier(
    n_estimators=100,    # Nombre d'arbres
    max_depth=None,      # Profondeur max (None = illimit√©e)
    random_state=cfg.seed,
    n_jobs=-1            # Utiliser tous les CPU
)
```

**Si model_type == "knn"** :
```python
from sklearn.neighbors import KNeighborsClassifier

return KNeighborsClassifier(
    n_neighbors=5,       # Nombre de voisins
    weights='distance',  # Pond√©ration par distance
    n_jobs=-1
)
```

---

### **Classe : MLModelBundle**

**Responsabilit√©** : Encapsuler tout ce qui est n√©cessaire pour sauvegarder/charger un mod√®le ML.

---

#### **Attributs**

**Ce qu'elle doit contenir** :
- `model` : Mod√®le sklearn entra√Æn√©
- `scaler` : StandardScaler (pour normaliser les features)
- `labels` : LabelEncoder (mapping classe ‚Üî id)
- `feature_extractor` : FeatureExtractor (pour extraire features)
- `cfg` : ModelConfig

---

#### **M√©thode : save(out_dir)**

**Ce qu'elle doit faire** :

**√âtape 1 : Cr√©er le dossier**
- S'assurer que `out_dir` existe

**√âtape 2 : Sauvegarder le mod√®le**
- Utiliser joblib : `joblib.dump(self.model, out_dir / "model.pkl")`

**√âtape 3 : Sauvegarder le scaler**
- Utiliser joblib : `joblib.dump(self.scaler, out_dir / "scaler.pkl")`

**√âtape 4 : Sauvegarder les labels**
- Convertir en dict : `labels_dict = self.labels.to_json_dict()`
- √âcrire en JSON : `json.dump(labels_dict, open(out_dir / "labels.json", "w"))`

**√âtape 5 : Sauvegarder la config**
- Convertir cfg en dict
- √âcrire en JSON : `json.dump(config_dict, open(out_dir / "config.json", "w"))`

**√âtape 6 : Sauvegarder la config des features**
- Informations sur les transformations utilis√©es
- √âcrire en JSON : `json.dump(feature_config, open(out_dir / "feature_config.json", "w"))`

---

#### **M√©thode : load(in_dir)** (classmethod)

**Ce qu'elle doit faire** :

**√âtape 1 : Charger le mod√®le**
- `model = joblib.load(in_dir / "model.pkl")`

**√âtape 2 : Charger le scaler**
- `scaler = joblib.load(in_dir / "scaler.pkl")`

**√âtape 3 : Charger les labels**
- Lire le JSON
- Cr√©er un LabelEncoder : `labels = LabelEncoder.from_json_dict(data)`

**√âtape 4 : Charger la config**
- Lire le JSON
- Cr√©er un ModelConfig

**√âtape 5 : Recr√©er le FeatureExtractor**
- Cr√©er un TransformationEngine avec les 6 transformations
- Cr√©er un FeatureExtractor avec ce moteur

**√âtape 6 : Cr√©er et retourner le bundle**
- `return MLModelBundle(model, scaler, labels, feature_extractor, cfg)`

---

#### **M√©thode : predict(features)**

**Ce qu'elle doit faire** :

**Param√®tres** :
- `features` : np.ndarray de shape `(n_features,)` ou `(1, n_features)`

**Retour** :
- `pred_id` : int (ID de la classe pr√©dite)
- `probs` : Dict[str, float] (probabilit√©s par classe)

---

**√âtape 1 : Reshape si n√©cessaire**
- Si shape `(n_features,)` : reshape en `(1, n_features)`

**√âtape 2 : Normaliser**
- `features_scaled = self.scaler.transform(features)`

**√âtape 3 : Pr√©dire**
- `pred_id = self.model.predict(features_scaled)[0]`

**√âtape 4 : Obtenir les probabilit√©s**
- Si le mod√®le supporte `predict_proba` :
  - `probs_array = self.model.predict_proba(features_scaled)[0]`
  - Cr√©er un dict : `{self.labels.decode(i): float(p) for i, p in enumerate(probs_array)}`
- Sinon :
  - `probs = {self.labels.decode(pred_id): 1.0}`

**√âtape 5 : Retourner**
- `return pred_id, probs`

---

<a id="train-pipeline"></a>
## 10. leaffliction/train_pipeline.py ‚Äî Pipeline d'entra√Ænement

### Changements par rapport √† CNN

**Pipeline ML Traditionnel** :
```
1. Scanner dataset
2. Split train/valid
3. Augmenter train set (images physiques)
4. Extraire features (train + valid)
5. Normaliser features
6. Entra√Æner mod√®le ML
7. √âvaluer
8. Sauvegarder
```

---

### **Classe : MLTrainer**

**Responsabilit√©** : Orchestrer tout le processus d'entra√Ænement ML.

---

#### **M√©thode : train(dataset_dir, out_dir, cfg)**

**Ce qu'elle doit faire** :

**Param√®tres** :
- `dataset_dir` : Path vers le dataset
- `out_dir` : Path vers le dossier de sortie
- `cfg` : TrainConfig

**Retour** :
- `Metrics` (train_accuracy, valid_accuracy, valid_count)

---

**√âtape 1 : Scanner le dataset**
```python
scanner = DatasetScanner()
index = scanner.scan(dataset_dir)
```

---

**√âtape 2 : Fitter le LabelEncoder**
```python
labels = LabelEncoder()
labels.fit(index.class_names)
```

---

**√âtape 3 : Split train/valid**
```python
splitter = DatasetSplitter()
train_items, valid_items = splitter.split(
    index.items,
    cfg.valid_ratio,
    cfg.seed,
    stratified=True
)
```

---

**√âtape 4 : Augmenter le train set (optionnel)**
```python
if cfg.augment_train:
    aug_engine = AugmentationEngine.default_six()
    train_items = aug_engine.augment_dataset(
        train_items,
        out_dir / "augmented",
        augmentations_per_image=3
    )
```

**R√©sultat** : `train_items` contient maintenant les originales + les augment√©es.

---

**√âtape 5 : Extraire les features**
```python
feature_extractor = FeatureExtractor(
    TransformationEngine.default_six().tfs
)

print("Extracting train features...")
X_train, y_train = feature_extractor.extract_batch(train_items)

print("Extracting validation features...")
X_valid, y_valid = feature_extractor.extract_batch(valid_items)
```

**R√©sultat** :
- `X_train` : shape `(n_train, n_features)`
- `y_train` : shape `(n_train,)`
- `X_valid` : shape `(n_valid, n_features)`
- `y_valid` : shape `(n_valid,)`

---

**√âtape 6 : Normaliser les features**
```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_valid_scaled = scaler.transform(X_valid)
```

**Important** : `fit_transform` sur train, `transform` sur valid (pas de data leakage).

---

**√âtape 7 : Construire le mod√®le**
```python
model_factory = MLModelFactory()
model = model_factory.build(cfg, model_type="svm")
```

---

**√âtape 8 : Entra√Æner**
```python
print("Training model...")
model.fit(X_train_scaled, y_train)
```

---

**√âtape 9 : √âvaluer**
```python
train_acc = model.score(X_train_scaled, y_train)
valid_acc = model.score(X_valid_scaled, y_valid)

metrics = Metrics(
    train_accuracy=train_acc,
    valid_accuracy=valid_acc,
    valid_count=len(valid_items)
)
```

---

**√âtape 10 : Sauvegarder le bundle**
```python
bundle = MLModelBundle(
    model=model,
    scaler=scaler,
    labels=labels,
    feature_extractor=feature_extractor,
    cfg=ModelConfig(num_classes=index.num_classes, seed=cfg.seed)
)
bundle.save(out_dir / "model")
```

---

**√âtape 11 : Retourner les m√©triques**
```python
return metrics
```

---

### **Classe : RequirementsGate**

**Identique √† la version CNN**, pas de changement.

---

### **Classe : TrainingPackager**

**Identique √† la version CNN**, pas de changement.

---

<a id="predict-pipeline"></a>
## 11. leaffliction/predict_pipeline.py ‚Äî Pipeline de pr√©diction

### Changements par rapport √† CNN

**Pipeline ML Traditionnel** :
```
1. Charger le bundle (model.pkl, scaler.pkl, labels.json)
2. Extraire features de l'image
3. Normaliser features
4. Pr√©dire avec le mod√®le ML
5. D√©coder le label
6. (Optionnel) Afficher transformations
```

---

### **Classe : MLPredictor**

**Responsabilit√©** : Charger le mod√®le et pr√©dire sur une image.

---

#### **M√©thode : predict(bundle_zip, image_path, cfg)**

**Ce qu'elle doit faire** :

**Param√®tres** :
- `bundle_zip` : Path vers learnings.zip
- `image_path` : Path vers l'image √† pr√©dire
- `cfg` : PredictConfig

**Retour** :
- `predicted_label` : str (nom de la classe)
- `probs` : Dict[str, float] (probabilit√©s par classe)

---

**√âtape 1 : Charger le bundle**
```python
import tempfile

with tempfile.Temporary
