# üìñ Guide Conceptuel d'Impl√©mentation ‚Äî Leaffliction

> **Objectif** : Expliquer **litt√©ralement** ce que chaque classe doit faire, **sans code**, pour que vous puissiez impl√©menter vous-m√™me.

---

## üìë Table des mati√®res

1. [leaffliction/cli.py ‚Äî Parsers d'arguments](#cli)
2. [leaffliction/utils.py ‚Äî Utilitaires](#utils)
3. [leaffliction/dataset.py ‚Äî Gestion du dataset](#dataset)
4. [leaffliction/plotting.py ‚Äî Visualisations](#plotting)
5. [leaffliction/augmentations.py ‚Äî Augmentations](#augmentations)
6. [leaffliction/transformations.py ‚Äî Transformations](#transformations)
7. [leaffliction/model.py ‚Äî Mod√®le et encodage](#model)
8. [leaffliction/train_pipeline.py ‚Äî Pipeline d'entra√Ænement](#train-pipeline)
9. [leaffliction/predict_pipeline.py ‚Äî Pipeline de pr√©diction](#predict-pipeline)

---


## üîÑ Ordre de r√©alisation et d√©pendances

### üìä Graphe de d√©pendances

```
Phase 1 (Fondations - Aucune d√©pendance)
‚îú‚îÄ‚îÄ cli.py          [Personne A] ‚è±Ô∏è 1-2h
‚îî‚îÄ‚îÄ utils.py        [Personne B] ‚è±Ô∏è 2-3h

Phase 2 (Dataset - D√©pend de utils.py)
‚îú‚îÄ‚îÄ dataset.py      [Personne A] ‚è±Ô∏è 4-5h (d√©pend: utils.py)
‚îî‚îÄ‚îÄ plotting.py     [Personne B] ‚è±Ô∏è 2-3h (ind√©pendant)

Phase 3 (Transformations - Peuvent √™tre parall√®les)
‚îú‚îÄ‚îÄ augmentations.py      [Personne A] ‚è±Ô∏è 3-4h (d√©pend: utils.py)
‚îî‚îÄ‚îÄ transformations.py    [Personne B] ‚è±Ô∏è 3-4h (ind√©pendant)

Phase 4 (Mod√®le - D√©pend de dataset.py)
‚îî‚îÄ‚îÄ model.py        [Personne A ou B] ‚è±Ô∏è 3-4h (d√©pend: dataset.py)

Phase 5 (Pipelines - D√©pend de tout)
‚îú‚îÄ‚îÄ train_pipeline.py     [Personne A] ‚è±Ô∏è 5-6h (d√©pend: dataset, model, augmentations)
‚îî‚îÄ‚îÄ predict_pipeline.py   [Personne B] ‚è±Ô∏è 2-3h (d√©pend: model, transformations)

Phase 6 (Scripts racine - D√©pend de tout)
‚îú‚îÄ‚îÄ Distribution.py       [Personne A] ‚è±Ô∏è 30min (d√©pend: cli, dataset, plotting)
‚îú‚îÄ‚îÄ Augmentation.py       [Personne B] ‚è±Ô∏è 30min (d√©pend: cli, augmentations, plotting)
‚îú‚îÄ‚îÄ Transformation.py     [Personne A] ‚è±Ô∏è 30min (d√©pend: cli, transformations, plotting)
‚îú‚îÄ‚îÄ train.py             [Personne B] ‚è±Ô∏è 1h (d√©pend: cli, train_pipeline, utils)
‚îî‚îÄ‚îÄ predict.py           [Personne A] ‚è±Ô∏è 30min (d√©pend: cli, predict_pipeline)
```

---

### üéØ Strat√©gie de travail en √©quipe (2 personnes)

#### **Option 1 : Division par couches (Recommand√©)**

**Personne A : Backend/ML**
- Phase 1 : `cli.py`
- Phase 2 : `dataset.py`
- Phase 3 : `augmentations.py`
- Phase 4 : `model.py`
- Phase 5 : `train_pipeline.py`
- Phase 6 : `Distribution.py`, `Transformation.py`, `predict.py`

**Personne B : Visualisation/Transformations**
- Phase 1 : `utils.py`
- Phase 2 : `plotting.py`
- Phase 3 : `transformations.py`
- Phase 4 : Aide sur `model.py` ou tests
- Phase 5 : `predict_pipeline.py`
- Phase 6 : `Augmentation.py`, `train.py`

**Avantages** :
- ‚úÖ S√©paration claire des responsabilit√©s
- ‚úÖ Peu de conflits de merge
- ‚úÖ Chacun devient expert de sa partie

---

#### **Option 2 : Division par fonctionnalit√©s**

**Personne A : Partie 1-2-3 du sujet**
- `cli.py` (parsers Distribution, Augmentation, Transformation)
- `utils.py`
- `dataset.py`
- `plotting.py`
- `augmentations.py`
- `transformations.py`
- Scripts : `Distribution.py`, `Augmentation.py`, `Transformation.py`

**Personne B : Partie 4 du sujet (Classification)**
- `cli.py` (parsers Train, Predict)
- `model.py`
- `train_pipeline.py`
- `predict_pipeline.py`
- Scripts : `train.py`, `predict.py`

**Avantages** :
- ‚úÖ Chacun peut tester sa partie ind√©pendamment
- ‚úÖ Correspond √† la structure du sujet
- ‚ö†Ô∏è N√©cessite de bien coordonner `cli.py`

---

### üìã Tableau de d√©pendances d√©taill√©

| Fichier | D√©pend de | Peut √™tre fait en parall√®le avec | Temps estim√© |
|---------|-----------|----------------------------------|--------------|
| **cli.py** | Rien | utils.py | 1-2h |
| **utils.py** | Rien | cli.py | 2-3h |
| **dataset.py** | utils.py | plotting.py | 4-5h |
| **plotting.py** | Rien | dataset.py, augmentations.py, transformations.py | 2-3h |
| **augmentations.py** | utils.py | transformations.py, plotting.py | 3-4h |
| **transformations.py** | Rien | augmentations.py, plotting.py, dataset.py | 3-4h |
| **model.py** | dataset.py (pour LabelEncoder) | Rien (bloquant pour pipelines) | 3-4h |
| **train_pipeline.py** | dataset.py, model.py, augmentations.py | predict_pipeline.py | 5-6h |
| **predict_pipeline.py** | model.py, transformations.py | train_pipeline.py | 2-3h |
| **Distribution.py** | cli.py, dataset.py, plotting.py | Autres scripts | 30min |
| **Augmentation.py** | cli.py, augmentations.py, plotting.py | Autres scripts | 30min |
| **Transformation.py** | cli.py, transformations.py, plotting.py | Autres scripts | 30min |
| **train.py** | cli.py, train_pipeline.py, utils.py | predict.py | 1h |
| **predict.py** | cli.py, predict_pipeline.py | train.py | 30min |

---

### ‚ö° Fichiers qui PEUVENT √™tre faits en parall√®le

**Groupe 1 (Phase 1 - Aucune d√©pendance)** :
- `cli.py` ‚ö° `utils.py`

**Groupe 2 (Phase 2)** :
- `plotting.py` ‚ö° `dataset.py` (si utils.py est termin√©)

**Groupe 3 (Phase 3 - Maximum de parall√©lisme)** :
- `augmentations.py` ‚ö° `transformations.py` ‚ö° `plotting.py` (si pas encore fait)

**Groupe 4 (Phase 5)** :
- `train_pipeline.py` ‚ö° `predict_pipeline.py` (si model.py est termin√©)

**Groupe 5 (Phase 6 - Tous les scripts racine)** :
- `Distribution.py` ‚ö° `Augmentation.py` ‚ö° `Transformation.py` ‚ö° `train.py` ‚ö° `predict.py`

---

### üö® Fichiers BLOQUANTS (√† faire en priorit√©)

Ces fichiers bloquent beaucoup d'autres :

1. **utils.py** (bloque : dataset.py, augmentations.py)
2. **dataset.py** (bloque : model.py, train_pipeline.py)
3. **model.py** (bloque : train_pipeline.py, predict_pipeline.py)

**Strat√©gie** : Commencer par ces 3 fichiers dans l'ordre !

---

### üìÖ Planning sugg√©r√© pour 2 personnes (sur 3-4 jours)

#### **Jour 1 : Fondations (6-8h)**
- **Matin** :
  - Personne A : `cli.py` (2h)
  - Personne B : `utils.py` (3h)
- **Apr√®s-midi** :
  - Personne A : `dataset.py` (4h)
  - Personne B : `plotting.py` (2h) + d√©but `transformations.py` (2h)

#### **Jour 2 : Transformations et Mod√®le (6-8h)**
- **Matin** :
  - Personne A : Finir `dataset.py` + d√©but `model.py` (4h)
  - Personne B : Finir `transformations.py` (2h) + `augmentations.py` (3h)
- **Apr√®s-midi** :
  - Personne A : Finir `model.py` (2h)
  - Personne B : Tests des transformations (2h)

#### **Jour 3 : Pipelines (6-8h)**
- **Matin** :
  - Personne A : `train_pipeline.py` (5h)
  - Personne B : `predict_pipeline.py` (3h)
- **Apr√®s-midi** :
  - Personne A : Finir `train_pipeline.py` (2h)
  - Personne B : Tests de pr√©diction (2h)

#### **Jour 4 : Scripts racine et tests (4-6h)**
- **Matin** :
  - Personne A : `Distribution.py`, `Transformation.py`, `predict.py` (2h)
  - Personne B : `Augmentation.py`, `train.py` (2h)
- **Apr√®s-midi** :
  - Les deux : Tests complets, debugging, g√©n√©ration signature.txt (2-4h)

---

### üí° Conseils pour le travail en √©quipe

**Communication** :
- üì± Utiliser un canal de communication rapide (Discord, Slack, etc.)
- üìù Documenter les interfaces entre fichiers (signatures de fonctions)
- üîÑ Faire des points r√©guliers (matin et soir)

**Git** :
- üåø Cr√©er une branche par fichier : `feature/cli`, `feature/utils`, etc.
- üîÄ Merger r√©guli√®rement pour √©viter les gros conflits
- ‚úÖ Faire des commits atomiques avec des messages clairs

**Tests** :
- üß™ Tester chaque fichier individuellement avant de merger
- üìä Cr√©er des donn√©es de test minimales
- üîç Valider les interfaces entre fichiers

**R√©partition des t√¢ches** :
- üìã Utiliser un Trello/Notion pour suivre l'avancement
- ‚è∞ Estimer le temps pour chaque t√¢che
- üéØ Prioriser les fichiers bloquants

---

### üéì Points de synchronisation obligatoires

**Sync 1 : Apr√®s Phase 1** (cli.py + utils.py)
- Valider les signatures des fonctions
- S'assurer que PathManager fonctionne
- Tester les parsers

**Sync 2 : Apr√®s Phase 2** (dataset.py + plotting.py)
- Valider DatasetIndex
- Tester le scan d'un petit dataset
- V√©rifier les visualisations

**Sync 3 : Apr√®s Phase 4** (model.py)
- Valider LabelEncoder
- Tester la construction du mod√®le
- S'assurer que ModelBundle fonctionne

**Sync 4 : Avant Phase 6** (tous les pipelines)
- Test end-to-end du training
- Test end-to-end de la pr√©diction
- Validation des contraintes (accuracy > 90%)

---

<a id="cli"></a>
## 1. leaffliction/cli.py ‚Äî Parsers d'arguments

### **Classe : CLIBuilder**

**Responsabilit√©** : Centraliser la cr√©ation de tous les parsers d'arguments pour √©viter la duplication.

---

#### **M√©thode : build_distribution_parser()**

**Ce qu'elle doit faire** :
1. Cr√©er un parser argparse avec une description claire
2. Ajouter UN argument positionnel obligatoire nomm√© "dataset_dir"
3. Cet argument doit accepter un string (chemin vers le dossier)
4. Ajouter un message d'aide pour expliquer √† quoi sert cet argument
5. Retourner le parser configur√©

**Utilisation attendue** : `python Distribution.py ./leaves/images/`

---

#### **M√©thode : build_augmentation_parser()**

**Ce qu'elle doit faire** :
1. Cr√©er un parser argparse
2. Ajouter UN argument positionnel obligatoire nomm√© "image_path"
3. Cet argument doit accepter un string (chemin vers une image)
4. Retourner le parser

**Utilisation attendue** : `python Augmentation.py "./leaves/images/Apple_healthy/image (1).JPG"`

---

#### **M√©thode : build_transformation_parser()**

**Ce qu'elle doit faire** :
1. Cr√©er un parser argparse
2. Ajouter un argument positionnel OPTIONNEL "image_path" (nargs="?")
   - Cet argument est utilis√© pour le mode single image
3. Ajouter un argument optionnel "-src" (type string)
   - Pour sp√©cifier le dossier source en mode batch
4. Ajouter un argument optionnel "-dst" (type string)
   - Pour sp√©cifier le dossier destination en mode batch
5. Ajouter un flag "-mask" (action="store_true")
   - Bool√©en pour appliquer des transformations de masque
6. Ajouter un flag "-recursive" (action="store_true", default=True)
   - Pour traiter les sous-dossiers r√©cursivement
7. Retourner le parser

**Utilisation attendue** :
- Mode single : `python Transformation.py "image.jpg"`
- Mode batch : `python Transformation.py -src ./dossier/ -dst ./sortie/`

---

#### **M√©thode : build_train_parser()**

**Ce qu'elle doit faire** :
1. Cr√©er un parser argparse
2. Ajouter UN argument positionnel obligatoire "dataset_dir"
3. Ajouter TOUS ces arguments optionnels avec leurs valeurs par d√©faut :
   - `--epochs` : entier, d√©faut 10
   - `--batch_size` : entier, d√©faut 32
   - `--lr` : float, d√©faut 0.001
   - `--valid_ratio` : float, d√©faut 0.2
   - `--seed` : entier, d√©faut 42
   - `--img_h` : entier, d√©faut 224
   - `--img_w` : entier, d√©faut 224
   - `--augment` : bool√©en (store_true), d√©faut True
   - `--export_images` : bool√©en (store_true), d√©faut True
   - `--out_dir` : string, d√©faut "artifacts"
   - `--out_zip` : string, d√©faut "learnings.zip"
4. Retourner le parser

**Utilisation attendue** : `python train.py ./leaves/images/ --epochs 20 --batch_size 32`

---

#### **M√©thode : build_predict_parser()**

**Ce qu'elle doit faire** :
1. Cr√©er un parser argparse
2. Ajouter DEUX arguments positionnels obligatoires :
   - "bundle_zip" : chemin vers learnings.zip
   - "image_path" : chemin vers l'image √† pr√©dire
3. Ajouter des arguments optionnels :
   - `--show_transforms` : bool√©en (store_true), d√©faut True
   - `--top_k` : entier, d√©faut 1
4. Retourner le parser

**Utilisation attendue** : `python predict.py learnings.zip "./image.jpg"`

---

**Pourquoi centraliser** : Si tu dois changer un argument (ex: renommer, changer la valeur par d√©faut), tu le changes √† UN SEUL endroit au lieu de modifier 5 fichiers diff√©rents.

---

<a id="utils"></a>
## 2. leaffliction/utils.py ‚Äî Utilitaires

### **Classe 1 : PathManager**

**Responsabilit√©** : G√©rer toutes les op√©rations li√©es aux chemins de fichiers et aux conventions de nommage.

---

#### **Attribut de classe : IMAGE_EXTS**

**Ce qu'il doit contenir** :
- Un ensemble (set) de toutes les extensions d'images support√©es
- Inclure les versions minuscules ET majuscules
- Exemples : ".jpg", ".JPG", ".jpeg", ".JPEG", ".png", ".PNG", ".bmp", ".tif", ".tiff", ".webp"

**Pourquoi un set** : Recherche O(1) pour v√©rifier si une extension est support√©e.

---

#### **M√©thode : ft_ensure_dir(path)**

**Ce qu'elle doit faire** :
1. Recevoir un objet Path (chemin vers un dossier)
2. V√©rifier si ce dossier existe
3. Si le dossier n'existe pas :
   - Le cr√©er
   - Cr√©er TOUS les dossiers parents n√©cessaires dans le chemin
4. Si le dossier existe d√©j√† :
   - Ne rien faire, ne pas lever d'erreur
5. Retourner le m√™me objet Path (pour permettre le cha√Ænage de m√©thodes)

**Exemple** : Si tu appelles `ft_ensure_dir(Path("a/b/c/d"))` et que seul "a" existe, la m√©thode doit cr√©er "b", "c", et "d".

**Pourquoi retourner path** : Permet d'√©crire `path = pm.ft_ensure_dir(Path("artifacts")).resolve()`

---

#### **M√©thode : ft_make_suffixed_path(image_path, suffix)**

**Ce qu'elle doit faire** :
1. Recevoir un chemin d'image (Path) et un suffixe (string)
2. Extraire le nom du fichier SANS l'extension (appel√© "stem")
   - Exemple : `image (1).JPG` ‚Üí stem = `"image (1)"`
3. Extraire l'extension du fichier
   - Exemple : `image (1).JPG` ‚Üí ext = `".JPG"`
4. Extraire le dossier parent
   - Exemple : `/path/to/image (1).JPG` ‚Üí parent = `/path/to/`
5. Construire un nouveau nom de fichier : `{stem}_{suffix}{extension}`
   - Exemple : `"image (1)"` + `"_Flip"` + `".JPG"` = `"image (1)_Flip.JPG"`
6. Combiner le parent avec le nouveau nom
7. Retourner le nouveau chemin complet

**Cas d'usage** : Sauvegarder les augmentations avec des suffixes comme "_Flip", "_Rotate", etc.

---

#### **M√©thode : ft_iter_images(root, recursive)**

**Ce qu'elle doit faire** :
1. Recevoir un dossier racine (Path) et un flag recursive (bool√©en)
2. Cr√©er une liste vide pour stocker les chemins d'images
3. D√©terminer le pattern de recherche :
   - Si recursive=True : chercher dans tous les sous-dossiers (`"**/*"`)
   - Si recursive=False : chercher seulement au niveau racine (`"*"`)
4. Pour CHAQUE extension dans IMAGE_EXTS :
   - Utiliser glob pour trouver tous les fichiers avec cette extension
   - Ajouter tous les chemins trouv√©s √† la liste
5. Trier la liste alphab√©tiquement
6. Retourner la liste tri√©e

**Pourquoi trier** : Garantit un ordre reproductible entre diff√©rentes ex√©cutions.

---

### **Classe 2 : Hasher**

**Responsabilit√©** : Calculer le hash SHA1 de fichiers pour g√©n√©rer signature.txt.

---

#### **M√©thode : ft_sha1_file(path, chunk_size)**

**Ce qu'elle doit faire** :
1. Recevoir un chemin de fichier (Path) et une taille de chunk (d√©faut 1MB = 1024*1024 bytes)
2. Initialiser un objet de hachage SHA1
3. Ouvrir le fichier en mode binaire lecture
4. Lire le fichier par morceaux (chunks) :
   - Lire chunk_size bytes
   - Mettre √† jour le hash avec ces bytes
   - R√©p√©ter jusqu'√† la fin du fichier
5. Fermer le fichier automatiquement
6. Obtenir le digest final du hash
7. Convertir le digest en format hexad√©cimal (string de 40 caract√®res)
8. Retourner cette string

**Pourquoi par chunks** : Un fichier ZIP peut faire plusieurs GB. Lire par chunks √©vite de saturer la RAM en chargeant tout le fichier d'un coup.

**Exemple de sortie** : `"7a18a838d2203cc7d6e8c4c521fdd4dd214aa560"`

---

### **Classe 3 : ZipPackager**

**Responsabilit√©** : Compresser des dossiers en fichiers ZIP et d√©compresser des ZIP.

---

#### **M√©thode : ft_zip_dir(src_dir, out_zip)**

**Ce qu'elle doit faire** :
1. Recevoir un dossier source (Path) et un chemin de sortie pour le ZIP (Path)
2. **V√âRIFICATION CRITIQUE** : S'assurer que out_zip n'est PAS √† l'int√©rieur de src_dir
   - Sinon, boucle infinie (le ZIP essaie de se compresser lui-m√™me)
3. Cr√©er un fichier ZIP en mode √©criture avec compression DEFLATED
4. Parcourir R√âCURSIVEMENT tous les fichiers dans src_dir :
   - Pour chaque fichier trouv√© :
     - Calculer son chemin RELATIF par rapport √† src_dir
     - Ajouter le fichier au ZIP avec ce chemin relatif (pas absolu)
5. Fermer le ZIP automatiquement

**Pourquoi chemin relatif** : Si tu utilises des chemins absolus, le ZIP contiendra des chemins comme `/home/user/project/file.txt` qui ne fonctionneront pas sur une autre machine.

**Exemple** :
- src_dir = `/home/user/artifacts/`
- Fichier = `/home/user/artifacts/models/model.keras`
- Chemin relatif dans le ZIP = `models/model.keras`

---

#### **M√©thode : ft_unzip(zip_path, extract_dir)** (optionnel mais utile)

**Ce qu'elle doit faire** :
1. Recevoir un chemin de ZIP (Path) et un dossier de destination (Path)
2. Ouvrir le ZIP en mode lecture
3. Extraire TOUT le contenu dans extract_dir
4. Pr√©server la structure des dossiers du ZIP
5. Fermer le ZIP automatiquement

---

<a id="dataset"></a>
## 3. leaffliction/dataset.py ‚Äî Gestion du dataset

### **Classe 1 : DatasetIndex (dataclass)**

**Responsabilit√©** : Repr√©senter l'index complet du dataset apr√®s scan. C'est une structure de donn√©es.

---

#### **Attributs obligatoires**

**Ce qu'elle doit contenir** :
1. `root` : Path vers le dossier racine du dataset
2. `class_names` : Liste des noms de classes (strings), tri√©e alphab√©tiquement
3. `items` : Liste de tuples `(chemin_image: Path, class_id: int)`
4. `counts` : Dictionnaire `{nom_classe: string ‚Üí nombre_images: int}`

**Exemple** :
```
DatasetIndex(
    root=Path("./leaves/images/"),
    class_names=["Apple_Black_rot", "Apple_healthy", "Grape_Black_rot"],
    items=[
        (Path("./leaves/images/Apple_Black_rot/image1.JPG"), 0),
        (Path("./leaves/images/Apple_Black_rot/image2.JPG"), 0),
        (Path("./leaves/images/Apple_healthy/image1.JPG"), 1),
        ...
    ],
    counts={
        "Apple_Black_rot": 252,
        "Apple_healthy": 150,
        "Grape_Black_rot": 180
    }
)
```

---

#### **Propri√©t√©s calcul√©es**

1. **num_classes**
   - Retourner la longueur de class_names
   - Exemple : 3 classes ‚Üí retourne 3

2. **size**
   - Retourner la longueur de items (nombre total d'images)
   - Exemple : 582 images ‚Üí retourne 582

**Pourquoi des propri√©t√©s** : √âvite de stocker des valeurs redondantes. Elles sont calcul√©es √† la demande.

---

### **Classe 2 : DatasetScanner**

**Responsabilit√©** : Scanner un dossier organis√© en sous-dossiers (un par classe) et construire un DatasetIndex.

---

#### **M√©thode : ft_scan(root)**

**Ce qu'elle doit faire** :

**√âtape 1 : Lister les sous-dossiers**
- Recevoir un chemin vers le dossier racine (Path)
- Lister TOUS les sous-dossiers directs (pas r√©cursif)
- Filtrer pour ne garder que les dossiers (pas les fichiers)

**√âtape 2 : Trier les dossiers**
- Trier la liste des dossiers alphab√©tiquement
- Cet ordre d√©termine les class_id (0, 1, 2, ...)

**√âtape 3 : Extraire les noms de classes**
- Pour chaque dossier, extraire son nom
- Ces noms deviennent class_names

**√âtape 4 : Scanner chaque classe**
- Pour chaque dossier (avec son index comme class_id) :
  - Lister toutes les images dans ce dossier
  - Pour chaque extension support√©e, chercher les fichiers
  - Compter le nombre total d'images trouv√©es
  - Pour chaque image trouv√©e :
    - Cr√©er un tuple (chemin_image, class_id)
    - Ajouter ce tuple √† la liste items
  - Stocker le compte dans le dictionnaire counts

**√âtape 5 : Construire et retourner**
- Cr√©er un objet DatasetIndex avec toutes ces informations
- Retourner cet objet

**Structure attendue du dataset** :
```
root/
  Apple_Black_rot/     ‚Üê class_id = 0
    image (1).JPG
    image (2).JPG
  Apple_healthy/       ‚Üê class_id = 1
    image (1).JPG
  Grape_Black_rot/     ‚Üê class_id = 2
    image (1).JPG
```

**Pourquoi trier** : L'ordre alphab√©tique garantit que `Apple_Black_rot` aura toujours class_id=0, m√™me si tu relances le programme.

---

### **Classe 3 : DatasetSplitter**

**Responsabilit√©** : Diviser les donn√©es en ensembles train et validation de mani√®re stratifi√©e.

---

#### **M√©thode : ft_split(items, valid_ratio, seed, stratified)**

**Ce qu'elle doit faire** :

**Param√®tres** :
- `items` : Liste de tuples (Path, class_id)
- `valid_ratio` : Float entre 0 et 1 (ex: 0.2 = 20% en validation)
- `seed` : Entier pour la reproductibilit√©
- `stratified` : Bool√©en (True recommand√©)

**√âtape 0 : Fixer le seed**
- Fixer le seed du g√©n√©rateur al√©atoire pour reproductibilit√©

---

**Si stratified=False** (simple, NON recommand√©) :

**√âtape 1** : Copier la liste items
**√âtape 2** : M√©langer al√©atoirement toute la liste
**√âtape 3** : Calculer l'index de s√©paration
- `split_idx = int(len(items) * (1 - valid_ratio))`
**√âtape 4** : S√©parer
- train = items[:split_idx]
- valid = items[split_idx:]
**√âtape 5** : Retourner (train, valid)

**Probl√®me** : Si une classe a peu d'images, elle peut √™tre absente de la validation.

---

**Si stratified=True** (RECOMMAND√â) :

**√âtape 1 : Grouper par classe**
- Cr√©er un dictionnaire vide : `{class_id: []}`
- Pour chaque item dans items :
  - Extraire le class_id
  - Ajouter l'item √† la liste correspondante dans le dictionnaire
- R√©sultat : `{0: [items de classe 0], 1: [items de classe 1], ...}`

**√âtape 2 : Splitter chaque classe s√©par√©ment**
- Cr√©er deux listes vides : train_items et valid_items
- Pour CHAQUE classe dans le dictionnaire :
  - R√©cup√©rer la liste des items de cette classe
  - Copier cette liste
  - M√©langer al√©atoirement cette copie
  - Calculer combien d'items vont en validation : `n_valid = int(len(liste) * valid_ratio)`
  - S√©parer : 
    - Les n_valid derniers items ‚Üí valid
    - Le reste ‚Üí train
  - Ajouter les items train de cette classe √† train_items
  - Ajouter les items valid de cette classe √† valid_items

**√âtape 3 : M√©langer les listes finales**
- M√©langer train_items (pour que les classes soient m√©lang√©es)
- M√©langer valid_items

**√âtape 4 : Retourner**
- Retourner (train_items, valid_items)

**Pourquoi stratifi√©** : Si une classe repr√©sente 10% du dataset, elle repr√©sentera aussi ~10% du train ET ~10% du valid. Cela garantit que toutes les classes sont pr√©sentes dans les deux ensembles.

**Exemple** :
- Classe A : 100 images ‚Üí 80 train, 20 valid
- Classe B : 50 images ‚Üí 40 train, 10 valid
- Classe C : 200 images ‚Üí 160 train, 40 valid

---

### **Classe 4 : TFDataConfig (dataclass)**

**Responsabilit√©** : Stocker la configuration pour construire un tf.data.Dataset.

---

#### **Attributs**

**Ce qu'elle doit contenir** :
- `img_size` : Tuple (hauteur, largeur) pour redimensionner les images (ex: (224, 224))
- `batch_size` : Entier, taille des batchs (ex: 32)
- `shuffle` : Bool√©en, m√©langer ou non les donn√©es
- `seed` : Entier, seed pour reproductibilit√©
- `cache` : Bool√©en, mettre en cache les donn√©es en RAM
- `prefetch` : Bool√©en, pr√©charger les donn√©es pendant le training

**Pourquoi une dataclass** : Regroupe tous les param√®tres de configuration dans un seul objet facile √† passer.

---

### **Classe 5 : TFDatasetBuilder**

**Responsabilit√©** : Construire un tf.data.Dataset optimis√© √† partir d'une liste d'items.

---

#### **M√©thode : __init__(cfg, augmentor)**

**Ce qu'elle doit faire** :
1. Recevoir une configuration (TFDataConfig)
2. Recevoir un augmenteur optionnel (peut √™tre None)
3. Stocker ces deux objets comme attributs

---

#### **M√©thode : build(items, training)**

**Ce qu'elle doit faire** :

**Param√®tres** :
- `items` : Liste de tuples (Path, class_id)
- `training` : Bool√©en (True pour train, False pour valid)

---

**√âtape 1 : Extraire paths et labels**
- Cr√©er une liste `paths` : extraire tous les chemins, convertir en strings
- Cr√©er une liste `labels` : extraire tous les class_id

---

**√âtape 2 : Cr√©er le dataset TensorFlow**
- Utiliser `tf.data.Dataset.from_tensor_slices((paths, labels))`
- Cela cr√©e un dataset qui yield des tuples (path_string, label_int)

---

**√âtape 3 : Shuffle (seulement si training=True)**
- Si training ET cfg.shuffle :
  - Appliquer shuffle avec buffer_size = nombre total d'items
  - Utiliser cfg.seed pour reproductibilit√©

**Pourquoi buffer_size = len(items)** : Garantit un m√©lange complet.

---

**√âtape 4 : Map load_and_preprocess**
- Appliquer la m√©thode `_load_and_preprocess` √† chaque √©l√©ment
- Utiliser `num_parallel_calls=AUTOTUNE` pour parall√©liser le chargement
- Cela transforme (path_string, label) ‚Üí (image_tensor, label)

---

**√âtape 5 : Map augmentations (seulement si training=True ET augmentor existe)**
- Si training ET self.augmentor n'est pas None :
  - Appliquer l'augmenteur √† chaque image
  - Utiliser `num_parallel_calls=AUTOTUNE`
  - Cela transforme (image, label) ‚Üí (image_augment√©e, label)

---

**√âtape 6 : Batch**
- Grouper les √©l√©ments par paquets de cfg.batch_size
- Cela transforme des √©l√©ments individuels en batchs
- Exemple : (image, label) ‚Üí (batch_images[32, 224, 224, 3], batch_labels[32])

---

**√âtape 7 : Cache (si cfg.cache=True)**
- Mettre en cache les donn√©es en RAM
- √âvite de recharger les images √† chaque epoch
- **Attention** : Utiliser seulement si le dataset tient en RAM

---

**√âtape 8 : Prefetch**
- Si cfg.prefetch=True :
  - Pr√©charger les donn√©es pendant que le GPU travaille
  - Utiliser `AUTOTUNE` pour optimisation automatique

**Pourquoi prefetch** : Pendant que le GPU entra√Æne sur le batch N, le CPU pr√©pare le batch N+1.

---

**√âtape 9 : Retourner**
- Retourner le dataset final configur√©

---

#### **M√©thode : _load_and_preprocess(path, label)**

**Ce qu'elle doit faire** :

**Param√®tres** :
- `path` : Tensor string (chemin vers l'image)
- `label` : Tensor int (class_id)

---

**√âtape 1 : Lire le fichier**
- Utiliser `tf.io.read_file(path)` pour lire les bytes du fichier
- Retourne un tensor de bytes

---

**√âtape 2 : D√©coder l'image**
- Utiliser `tf.image.decode_jpeg` avec channels=3
- Cela d√©code les bytes JPEG en tensor RGB
- R√©sultat : tensor de shape (H_original, W_original, 3) avec valeurs [0, 255]

---

**√âtape 3 : Redimensionner**
- Utiliser `tf.image.resize` pour redimensionner √† cfg.img_size
- Exemple : (1024, 768, 3) ‚Üí (224, 224, 3)

---

**√âtape 4 : Normaliser**
- Convertir en float32
- Diviser par 255.0
- R√©sultat : valeurs entre [0, 1]

**Pourquoi normaliser** : Les r√©seaux de neurones fonctionnent mieux avec des valeurs entre 0 et 1.

---

**√âtape 5 : Retourner**
- Retourner le tuple (image_normalis√©e, label)

---

**Pourquoi ce pipeline** : C'est le pipeline standard TensorFlow optimis√© pour les performances. Chaque √©tape a un r√¥le pr√©cis et l'ordre est important.

---

<a id="plotting"></a>
## 4. leaffliction/plotting.py ‚Äî Visualisations

### **Classe 1 : DistributionPlotter**

**Responsabilit√©** : Afficher des graphiques pour visualiser la distribution des classes dans le dataset.

---

#### **M√©thode : plot_pie(counts, title, save_to)**

**Ce qu'elle doit faire** :

**Param√®tres** :
- `counts` : Dictionnaire {nom_classe: nombre_images}
- `title` : String, titre du graphique
- `save_to` : Path optionnel pour sauvegarder

---

**√âtape 1 : Extraire les donn√©es**
- Cr√©er une liste `labels` avec les cl√©s du dictionnaire (noms de classes)
- Cr√©er une liste `sizes` avec les valeurs du dictionnaire (nombres)

---

**√âtape 2 : Cr√©er la figure**
- Cr√©er une nouvelle figure matplotlib
- Taille recommand√©e : 10x8 pouces

---

**√âtape 3 : Dessiner le pie chart**
- Utiliser `plt.pie` avec :
  - `sizes` comme valeurs (d√©termine la taille des parts)
  - `labels` comme √©tiquettes
  - `autopct='%1.1f%%'` pour afficher les pourcentages (ex: "25.5%")
  - `startangle=90` pour commencer √† 12h (haut)
  - Couleurs distinctes pour chaque classe (utiliser un colormap)

---

**√âtape 4 : Ajouter le titre**
- Utiliser `plt.title` avec le titre fourni
- Style recommand√© : fontsize=14, fontweight='bold'

---

**√âtape 5 : Rendre le cercle parfait**
- Utiliser `plt.axis('equal')` pour que le pie chart soit un cercle et pas une ellipse

---

**√âtape 6 : Sauvegarder si demand√©**
- Si save_to n'est pas None :
  - Sauvegarder la figure en haute r√©solution (dpi=300)
  - Utiliser bbox_inches='tight' pour √©viter de couper les labels

---

**√âtape 7 : Afficher**
- Utiliser `plt.show()` pour afficher la figure

---

#### **M√©thode : plot_bar(counts, title, save_to)**

**Ce qu'elle doit faire** :

**Param√®tres** : Identiques √† plot_pie

---

**√âtape 1 : Extraire les donn√©es**
- Identique √† plot_pie

---

**√âtape 2 : Cr√©er la figure**
- Taille recommand√©e : 12x6 pouces (plus large pour les barres)

---

**√âtape 3 : Dessiner les barres**
- Utiliser `plt.bar` avec :
  - `labels` comme positions X
  - `values` comme hauteurs
  - Couleur : 'skyblue' ou autre couleur agr√©able
  - Bordure : 'navy' pour contraste
  - Transparence : alpha=0.7

---

**√âtape 4 : Ajouter les valeurs au-dessus des barres**
- Pour chaque barre :
  - R√©cup√©rer sa hauteur
  - Afficher le nombre exact au-dessus de la barre
  - Centrer le texte horizontalement
  - Positionner verticalement juste au-dessus

**Pourquoi** : Permet de lire les valeurs exactes sans deviner.

---

**√âtape 5 : Configurer les axes**
- Label axe X : "Classes" (fontsize=12)
- Label axe Y : "Number of images" (fontsize=12)
- Rotation des labels X : 45¬∞ si nombreux, horizontal sinon
- Alignement : ha='right' si rotation
- Grille horizontale : alpha=0.3 pour faciliter la lecture

---

**√âtape 6 : Ajouter le titre**
- Identique √† plot_pie

---

**√âtape 7 : Ajuster le layout**
- Utiliser `plt.tight_layout()` pour √©viter que les labels se chevauchent

---

**√âtape 8 : Sauvegarder et afficher**
- Identique √† plot_pie

---

**Pourquoi deux graphiques** :
- **Pie chart** : Montre les proportions relatives (25% vs 30%)
- **Bar chart** : Montre les valeurs absolues (250 images vs 300 images)
- Les deux sont compl√©mentaires et donnent des insights diff√©rents

---

### **Classe 2 : GridPlotter**

**Responsabilit√©** : Afficher une grille d'images (original + variantes) de mani√®re organis√©e.

---

#### **M√©thode : show_grid(title, images, original, save_to, max_cols)**

**Ce qu'elle doit faire** :

**Param√®tres** :
- `title` : String, titre de la figure
- `images` : Dictionnaire {nom_variante: image_tensor}
- `original` : Image originale (optionnel, peut √™tre None)
- `save_to` : Path optionnel pour sauvegarder
- `max_cols` : Entier, nombre max de colonnes (d√©faut 3)

---

**√âtape 1 : Calculer le nombre total d'images**
- Compter les images dans le dictionnaire
- Ajouter 1 si original existe
- Exemple : 6 variantes + 1 original =
7 images

---

**√âtape 2 : Calculer le layout (rows, cols)**
- cols = minimum entre max_cols et total
  - Exemple : si total=7 et max_cols=3, alors cols=3
- rows = arrondi sup√©rieur de (total / cols)
  - Exemple : 7 images / 3 cols = 2.33 ‚Üí 3 rows

---

**√âtape 3 : Cr√©er la figure avec subplots**
- Cr√©er une grille de subplots (rows x cols)
- Taille adapt√©e : cols * 4 pouces de large, rows * 4 pouces de haut
  - Exemple : 3 cols √ó 3 rows = figure de 12x12 pouces

---

**√âtape 4 : Ajouter le titre principal**
- Utiliser `fig.suptitle` avec le titre fourni
- Style : fontsize=16, fontweight='bold'

---

**√âtape 5 : Aplatir les axes pour it√©ration facile**
- Si une seule image : axes devient une liste avec un √©l√©ment
- Si plusieurs : aplatir la grille 2D en liste 1D
- Cela permet d'it√©rer facilement avec un index

---

**√âtape 6 : Afficher l'original en premier (si existe)**
- Si original n'est pas None :
  - Utiliser `_show_image` sur le premier axe (index 0)
  - Titre : "Original"
  - Incr√©menter l'index

---

**√âtape 7 : Afficher toutes les variantes**
- Pour chaque (nom, image) dans le dictionnaire images :
  - Si l'index est encore valide (< nombre d'axes) :
    - Utiliser `_show_image` sur l'axe courant
    - Titre : le nom de la variante
    - Incr√©menter l'index

---

**√âtape 8 : D√©sactiver les axes inutilis√©s**
- Si rows * cols > total d'images :
  - Pour chaque axe restant (de index jusqu'√† la fin) :
    - D√©sactiver compl√®tement cet axe (axis('off'))

**Pourquoi** : √âvite d'avoir des cases vides avec des axes visibles.

---

**√âtape 9 : Ajuster le layout**
- Utiliser `plt.tight_layout()` pour espacer correctement les subplots

---

**√âtape 10 : Sauvegarder et afficher**
- Si save_to fourni : sauvegarder en haute r√©solution
- Afficher avec `plt.show()`

---

#### **M√©thode : _show_image(ax, img, title)**

**Ce qu'elle doit faire** :

**Param√®tres** :
- `ax` : Un axe matplotlib (subplot)
- `img` : Image (tensor ou numpy array)
- `title` : String, titre pour cette image

---

**√âtape 1 : Convertir le tensor en numpy si n√©cessaire**
- Si l'image a une m√©thode `.numpy()` : l'appeler
- Sinon : supposer que c'est d√©j√† un numpy array

---

**√âtape 2 : Clipper les valeurs entre [0, 1]**
- Utiliser `np.clip(img_np, 0, 1)`
- **Critique** : √âvite les erreurs d'affichage si des valeurs sortent de [0, 1]

---

**√âtape 3 : D√©terminer le type d'image**
- V√©rifier la shape de l'image :
  - Si 2D (H, W) : grayscale
  - Si 3D avec shape[-1] == 1 : grayscale avec channel
  - Si 3D avec shape[-1] == 3 : RGB

---

**√âtape 4 : Afficher avec le bon colormap**
- Si grayscale (2D) :
  - Utiliser `ax.imshow(img_np, cmap='gray')`
- Si grayscale avec channel (3D, 1 channel) :
  - Extraire le channel : `img_np[:, :, 0]`
  - Utiliser `ax.imshow(img_np[:, :, 0], cmap='gray')`
- Si RGB (3D, 3 channels) :
  - Utiliser `ax.imshow(img_np)` sans cmap

---

**√âtape 5 : Ajouter le titre**
- Utiliser `ax.set_title(title)` avec fontsize=12

---

**√âtape 6 : D√©sactiver les axes**
- Utiliser `ax.axis('off')` pour enlever les ticks et les bordures
- Rend l'affichage plus propre

---

**Pourquoi cette structure** : Elle permet d'afficher n'importe quel nombre d'images de mani√®re flexible, avec gestion automatique du layout et support de diff√©rents types d'images.

---

<a id="augmentations"></a>
## 5. leaffliction/augmentations.py ‚Äî Augmentations

### **Classe 1 : KerasAugmentationsFactory**

**Responsabilit√©** : Cr√©er un Sequential Keras contenant des layers d'augmentation pour le training.

---

#### **M√©thode : build()**

**Ce qu'elle doit faire** :
1. Cr√©er un objet `keras.Sequential`
2. Ajouter des layers d'augmentation Keras :
   - `RandomFlip("horizontal")` : flip horizontal al√©atoire
   - `RandomRotation(0.1)` : rotation al√©atoire ¬±10%
   - `RandomZoom(0.1)` : zoom al√©atoire ¬±10%
   - `RandomContrast(0.1)` : contraste al√©atoire
   - `RandomBrightness(0.1)` : luminosit√© al√©atoire
3. Retourner ce Sequential

**Utilisation** : Ce Sequential sera appliqu√© PENDANT le training dans le pipeline tf.data.

**Pourquoi Keras layers** : Ils s'ex√©cutent sur GPU, sont int√©gr√©s au graphe TensorFlow, et sont automatiquement d√©sactiv√©s en mode validation.

---

### **Classe 2 : AugmentationEngine**

**Responsabilit√©** : Appliquer des augmentations D√âTERMINISTES pour visualisation et sauvegarde.

---

#### **M√©thode : __init__(augs)**

**Ce qu'elle doit faire** :
1. Recevoir une liste d'objets Augmentation
2. Stocker cette liste comme attribut

---

#### **M√©thode : default_six()**

**Ce qu'elle doit faire** :
1. Cr√©er une liste contenant exactement 6 augmentations :
   - FlipHorizontalAug()
   - RotateAug(angle=15.0)
   - BrightnessContrastAug(brightness=0.3, contrast=0.0)
   - GaussianBlurAug(sigma=2.0)
   - RandomCropResizeAug(crop_ratio=0.8)
   - BrightnessContrastAug(brightness=0.0, contrast=0.5)
2. Cr√©er et retourner un AugmentationEngine avec cette liste

**Pourquoi 6** : Le sujet demande exactement 6 types d'augmentations.

---

#### **M√©thode : apply_all(img)**

**Ce qu'elle doit faire** :
1. Recevoir une image (tensor)
2. Cr√©er un dictionnaire vide pour les r√©sultats
3. Pour chaque augmentation dans self.augs :
   - Appliquer l'augmentation √† l'image
   - Stocker le r√©sultat dans le dictionnaire avec le nom de l'augmentation comme cl√©
4. Retourner le dictionnaire {nom: image_augment√©e}

**Exemple de retour** :
```
{
    "Flip": tensor_flipp√©,
    "Rotate": tensor_tourn√©,
    "Brightness": tensor_lumineux,
    ...
}
```

---

### **Les 6 Augmentations (classes individuelles)**

Chaque augmentation doit avoir :
- Un attribut `name` (string) pour identifier l'augmentation
- Une m√©thode `apply(img)` qui prend une image et retourne l'image augment√©e

---

#### **Classe : FlipHorizontalAug**

**Attributs** :
- `name` = "Flip"

**M√©thode apply(img)** :
1. Recevoir une image tensor
2. Appliquer un flip horizontal (miroir gauche-droite)
3. Utiliser `tf.image.flip_left_right(img)`
4. Retourner l'image flipp√©e

**Effet** : L'image est invers√©e horizontalement comme dans un miroir.

---

#### **Classe : RotateAug**

**Attributs** :
- `angle` : Float, angle de rotation en degr√©s (ex: 15.0)
- `name` = "Rotate"

**M√©thode apply(img)** :
1. Recevoir une image tensor
2. Convertir l'angle de degr√©s en radians : `angle_rad = angle * œÄ / 180`
3. Appliquer une rotation avec interpolation bilin√©aire
4. Utiliser `tfa.image.rotate` (TensorFlow Addons) ou √©quivalent
5. Retourner l'image tourn√©e

**Effet** : L'image est tourn√©e de X degr√©s dans le sens horaire ou anti-horaire.

---

#### **Classe : BrightnessContrastAug**

**Attributs** :
- `brightness` : Float, facteur de luminosit√© (ex: 0.3 = +30%)
- `contrast` : Float, facteur de contraste (ex: 0.5 = +50%)
- `name` = "Brightness" ou "Contrast" selon ce qui est modifi√©

**M√©thode apply(img)** :
1. Recevoir une image tensor
2. Si brightness != 0 :
   - Ajuster la luminosit√© : `img = tf.image.adjust_brightness(img, brightness)`
3. Si contrast != 0 :
   - Ajuster le contraste : `img = tf.image.adjust_contrast(img, 1 + contrast)`
4. Clipper les valeurs entre [0, 1]
5. Retourner l'image modifi√©e

**Effet** : L'image devient plus claire/sombre (brightness) ou plus/moins contrast√©e.

---

#### **Classe : GaussianBlurAug**

**Attributs** :
- `sigma` : Float, √©cart-type du flou gaussien (ex: 2.0)
- `name` = "Blur"

**M√©thode apply(img)** :
1. Recevoir une image tensor
2. Cr√©er un noyau gaussien avec le sigma donn√©
3. Appliquer une convolution 2D avec ce noyau
4. Utiliser `tfa.image.gaussian_filter2d` ou impl√©menter manuellement
5. Retourner l'image flout√©e

**Effet** : L'image devient floue, les d√©tails sont att√©nu√©s.

**Formule du noyau gaussien** :
```
G(x, y) = (1 / 2œÄœÉ¬≤) * exp(-(x¬≤ + y¬≤) / 2œÉ¬≤)
```

---

#### **Classe : RandomCropResizeAug**

**Attributs** :
- `crop_ratio` : Float entre 0 et 1 (ex: 0.8 = garder 80% de l'image)
- `name` = "Crop"

**M√©thode apply(img)** :
1. Recevoir une image tensor de shape (H, W, C)
2. Calculer la nouvelle taille apr√®s crop :
   - `new_h = int(H * crop_ratio)`
   - `new_w = int(W * crop_ratio)`
3. Calculer les offsets pour centrer le crop :
   - `offset_h = (H - new_h) // 2`
   - `offset_w = (W - new_w) // 2`
4. Extraire la r√©gion centrale : `img[offset_h:offset_h+new_h, offset_w:offset_w+new_w]`
5. Redimensionner √† la taille originale (H, W)
6. Utiliser `tf.image.resize` avec interpolation bilin√©aire
7. Retourner l'image cropp√©e et resiz√©e

**Effet** : Zoom sur le centre de l'image.

---

### **Classe 3 : AugmentationSaver**

**Responsabilit√©** : Sauvegarder les images augment√©es sur disque avec les bons noms.

---

#### **M√©thode : __init__(path_manager)**

**Ce qu'elle doit faire** :
1. Recevoir un objet PathManager
2. Stocker comme attribut

---

#### **M√©thode : save_all(image_path, results)**

**Ce qu'elle doit faire** :

**Param√®tres** :
- `image_path` : Path vers l'image originale
- `results` : Dictionnaire {nom_aug: image_tensor}

---

**√âtape 1 : Cr√©er une liste pour les chemins sauvegard√©s**

---

**√âtape 2 : Pour chaque augmentation dans results**
- Extraire le nom de l'augmentation et l'image tensor

**√âtape 3 : G√©n√©rer le chemin de sortie**
- Utiliser `path_manager.ft_make_suffixed_path(image_path, nom_aug)`
- Exemple : `image (1).JPG` + "Flip" ‚Üí `image (1)_Flip.JPG`

**√âtape 4 : Convertir le tensor en uint8**
- Multiplier par 255.0 : `img_tensor * 255.0`
- Caster en uint8 : `tf.cast(..., tf.uint8)`
- R√©sultat : valeurs entre [0, 255]

**√âtape 5 : Encoder en JPEG**
- Utiliser `tf.image.encode_jpeg(img_uint8, quality=95)`
- Retourne des bytes JPEG

**√âtape 6 : √âcrire sur disque**
- Utiliser `tf.io.write_file(str(out_path), encoded_jpeg)`

**√âtape 7 : Ajouter le chemin √† la liste**

---

**√âtape 8 : Retourner la liste des chemins sauvegard√©s**

---

**Pourquoi quality=95** : Bon compromis entre qualit√© et taille de fichier.

---

<a id="transformations"></a>
## 6. leaffliction/transformations.py ‚Äî Transformations

### **Classe 1 : TransformationEngine**

**Responsabilit√©** : Appliquer des transformations d√©terministes pour extraire des caract√©ristiques.

---

#### **M√©thode : __init__(tfs)**

**Ce qu'elle doit faire** :
1. Recevoir une liste d'objets Transformation
2. Stocker cette liste comme attribut

---

#### **M√©thode : default_six()**

**Ce qu'elle doit faire** :
1. Cr√©er une liste contenant exactement 6 transformations :
   - GrayscaleTf()
   - CannyEdgesTf()
   - HistogramEqualisationTf()
   - SharpenTf()
   - ThresholdTf()
   - MorphologyTf(mode='erode')
2. Cr√©er et retourner un TransformationEngine avec cette liste

---

#### **M√©thode : apply_all(img)**

**Ce qu'elle doit faire** :
1. Recevoir une image (tensor)
2. Cr√©er un dictionnaire vide pour les r√©sultats
3. Pour chaque transformation dans self.tfs :
   - Appliquer la transformation √† l'image
   - Stocker le r√©sultat dans le dictionnaire avec le nom comme cl√©
4. Retourner le dictionnaire {nom: image_transform√©e}

---

### **Les 6 Transformations (classes individuelles)**

Chaque transformation doit avoir :
- Un attribut `name` (string)
- Une m√©thode `apply(img)` qui prend une image et retourne l'image transform√©e

---

#### **Classe : GrayscaleTf**

**Attributs** :
- `name` = "Grayscale"

**M√©thode apply(img)** :
1. Recevoir une image RGB tensor (H, W, 3)
2. Convertir en niveaux de gris
3. Utiliser `tf.image.rgb_to_grayscale(img)`
4. R√©sultat : tensor (H, W, 1)
5. Retourner l'image en grayscale

**Formule** :
```
Gray = 0.299 * R + 0.587 * G + 0.114 * B
```

**Effet** : Supprime les informations de couleur, garde seulement l'intensit√© lumineuse.

---

#### **Classe : CannyEdgesTf**

**Attributs** :
- `low_threshold` : Entier (ex: 50)
- `high_threshold` : Entier (ex: 150)
- `name` = "Canny"

**M√©thode apply(img)** :

**√âtape 1 : D√©finir une fonction Python**
- Cr√©er une fonction qui prend un numpy array
- Convertir en uint8 [0, 255]
- Si RGB : convertir en grayscale avec OpenCV
- Appliquer `cv2.Canny(gray, low_threshold, high_threshold)`
- Retourner en float32 [0, 1]

**√âtape 2 : Wrapper avec tf.py_function**
- Utiliser `tf.py_function` pour appeler la fonction Python
- Sp√©cifier le type de sortie : tf.float32
- D√©finir la shape de sortie : (H, W, 1)

**√âtape 3 : Retourner le r√©sultat**

**Effet** : D√©tecte les contours dans l'image. R√©sultat binaire (blanc = contour, noir = fond).

**Algorithme Canny** :
1. Flou gaussien (r√©duction du bruit)
2. Calcul du gradient (Sobel)
3. Suppression des non-maxima
4. Seuillage par hyst√©r√©sis (low et high thresholds)

---

#### **Classe : HistogramEqualisationTf**

**Attributs** :
- `name` = "HistEq"

**M√©thode apply(img)** :

**√âtape 1 : D√©finir une fonction Python**
- Convertir en uint8 [0, 255]
- Si RGB : convertir en grayscale
- Appliquer `cv2.equalizeHist(gray)`
- Retourner en float32 [0, 1]

**√âtape 2 : Wrapper avec tf.py_function**

**√âtape 3 : Retourner le r√©sultat**

**Effet** : Am√©liore le contraste en redistribuant les intensit√©s de pixels.

**Formule** :
```
Pour chaque intensit√© i :
  cdf(i) = somme cumulative de l'histogramme jusqu'√† i
  nouvelle_valeur(i) = (cdf(i) - cdf_min) / (total_pixels - cdf_min) * 255
```

---

#### **Classe : SharpenTf**

**Attributs** :
- `name` = "Sharpen"

**M√©thode apply(img)** :

**√âtape 1 : D√©finir le noyau de convolution**
```
kernel = [[ 0, -1,  0],
          [-1,  5, -1],
          [ 0, -1,  0]]
```

**√âtape 2 : Convertir en tensor TensorFlow**
- Shape : (3, 3, 1, 1) pour convolution 2D

**√âtape 3 : Appliquer la convolution**
- Si RGB : appliquer sur chaque canal s√©par√©ment
- Utiliser `tf.nn.conv2d` avec padding='SAME'

**√âtape 4 : Retourner le r√©sultat**

**Effet** : Accentue les d√©tails et les contours de l'image.

**Principe** : Le noyau amplifie les diff√©rences entre pixels voisins.

---

#### **Classe : ThresholdTf**

**Attributs** :
- `threshold` : Float entre 0 et 1 (ex: 0.5)
- `name` = "Threshold"

**M√©thode apply(img)** :

**√âtape 1 : Convertir en grayscale si RGB**
- Si shape[-1] == 3 : utiliser `tf.image.rgb_to_grayscale`

**√âtape 2 : Appliquer le seuillage**
- Comparer chaque pixel au threshold
- Si pixel > threshold : 1.0 (blanc)
- Sinon : 0.0 (noir)
- Utiliser : `tf.cast(gray > threshold, tf.float32)`

**√âtape 3 : Retourner l'image binaire**

**Effet** : Segmentation binaire de l'image.

**Formule** :
```
output(x, y) = 1 si input(x, y) > threshold, sinon 0
```

---

#### **Classe : MorphologyTf**

**Attributs** :
- `mode` : String ("erode", "dilate", "open", "close")
- `kernel_size` : Entier (ex: 5)
- `name` = "Morphology"

**M√©thode apply(img)** :

**√âtape 1 : D√©finir une fonction Python**
- Convertir en uint8 [0, 255]
- Si RGB : convertir en grayscale
- Cr√©er un √©l√©ment structurant (kernel) :
  - Utiliser `cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (kernel_size, kernel_size))`
- Selon le mode :
  - "erode" : `cv2.erode(gray, kernel)`
  - "dilate" : `cv2.dilate(gray, kernel)`
  - "open" : `cv2.morphologyEx(gray, cv2.MORPH_OPEN, kernel)`
  - "close" : `cv2.morphologyEx(gray, cv2.MORPH_CLOSE, kernel)`
- Retourner en float32 [0, 1]

**√âtape 2 : Wrapper avec tf.py_function**

**√âtape 3 : Retourner le r√©sultat**

**Effet** :
- **√ârosion** : R√©duit les objets blancs, √©limine le bruit
- **Dilatation** : Agrandit les objets blancs, comble les trous
- **Opening** : √ârosion puis dilatation (enl√®ve le bruit)
- **Closing** : Dilatation puis √©rosion (comble les trous)

**Formules** :
```
√ârosion : output(x,y) = min{input(x+i, y+j) | (i,j) ‚àà kernel}
Dilatation : output(x,y) = max{input(x+i, y+j) | (i,j) ‚àà kernel}
```

---

### **Classe 2 : BatchTransformer**

**Responsabilit√©** : Appliquer des transformations √† tout un dossier d'images.

---

#### **M√©thode : __init__(engine, path_manager)**

**Ce qu'elle doit faire** :
1. Recevoir un TransformationEngine
2. Recevoir un PathManager
3. Stocker comme attributs

---

#### **M√©thode : run(src, dst, recursive)**

**Ce qu'elle doit faire** :

**Param√®tres** :
- `src` : Path vers le dossier source
- `dst` : Path vers le dossier destination
- `recursive` : Bool√©en

---

**√âtape 1 : Cr√©er le dossier destination**
- Utiliser `path_manager.ft_ensure_dir(dst)`

---

**√âtape 2 : Lister toutes les images**
- Utiliser `path_manager.ft_iter_images(src, recursive)`

---

**√âtape 3 : Pour chaque image**

**Sous-√©tape 3.1 : Charger l'image**
- Lire le fichier avec `tf.io.read_file`
- D√©coder avec `tf.image.decode_jpeg`
- Normaliser [0, 1]

**Sous-√©tape 3.2 : Appliquer toutes les transformations**
- Utiliser `engine.apply_all(img)`
- R√©sultat : dictionnaire {nom: image_transform√©e}

**Sous-√©tape 3.3 : Sauvegarder chaque transformation**
- Pour chaque (nom, img_tf) dans le dictionnaire :
  - Calculer le chemin relatif de l'image par rapport √† src
  - Cr√©er le m√™me chemin relatif dans dst
  - Ajouter le suffixe du nom de transformation
  - Cr√©er les dossiers n√©cessaires
  - Sauvegarder l'image transform√©e

---

**√âtape 4 : Afficher un message de progression**
- Optionnel : afficher combien d'images ont √©t√© trait√©es

---

<a id="model"></a>
## 7. leaffliction/model.py ‚Äî Mod√®le et encodage

### **Classe 1 : ModelConfig (dataclass)**

**Responsabilit√©** : Stocker la configuration du mod√®le.

---

#### **Attributs**

**Ce qu'elle doit contenir** :
- `img_size` : Tuple (hauteur, largeur) (ex: (224, 224))
- `num_classes` : Entier, nombre de classes
- `seed` : Entier pour reproductibilit√©
- `framework` : String, toujours "tf" pour TensorFlow
- `extra` : Dictionnaire pour param√®tres additionnels

---

### **Classe 2 : ModelPaths (dataclass)**

**Responsabilit√©** : D√©finir les noms de fichiers pour sauvegarder le mod√®le.

---

#### **Attributs**

**Ce qu'elle doit contenir** :
- `model_file` : String, nom du fichier mod√®le (ex: "model.keras")
- `labels_file` : String, nom du fichier labels (ex: "labels.json")
- `config_file` : String, nom du fichier config (ex: "config.json")
- `preprocess_file` : String, nom du fichier preprocess (ex: "preprocess.json")

---

### **Classe 3 : LabelEncoder**

**Responsabilit√©** : G√©rer le mapping bidirectionnel entre noms de classes et IDs.

---

#### **M√©thode : __init__()**

**Ce qu'elle doit faire** :
1. Cr√©er un dictionnaire vide `class_to_id` : {nom_classe ‚Üí id}
2. Cr√©er un dictionnaire vide `id_to_class` : {id ‚Üí nom_classe}

---

#### **M√©thode : fit(class_names)**

**Ce qu'elle doit faire** :
1. Recevoir une liste de noms de classes (tri√©e)
2. Pour chaque nom avec son index :
   - Ajouter au dictionnaire `class_to_id` : {nom ‚Üí index}
   - Ajouter au dictionnaire `id_to_class` : {index ‚Üí nom}

**Exemple** :
```
class_names = ["Apple_Black_rot", "Apple_healthy", "Grape_Black_rot"]

R√©sultat :
class_to_id = {
    "Apple_Black_rot": 0,
    "Apple_healthy": 1,
    "Grape_Black_rot": 2
}
id_to_class = {
    0: "Apple_Black_rot",
    1: "Apple_healthy",
    2: "Grape_Black_rot"
}
```

---

#### **M√©thode : encode(class_name)**

**Ce qu'elle doit faire** :
1. Recevoir un nom de classe (string)
2. Chercher dans `class_to_id`
3. Retourner l'ID correspondant
4. Si non trouv√© : lever une erreur

---

#### **M√©thode : decode(class_id)**

**Ce qu'elle doit faire** :
1. Recevoir un ID (entier)
2. Chercher dans `id_to_class`
3. Retourner le nom de classe correspondant
4. Si non trouv√© : lever une erreur

---

#### **M√©thode : to_json_dict()**

**Ce qu'elle doit faire** :
1. Cr√©er un dictionnaire avec :
   - "class_to_id" : self.class_to_id
   - "id_to_class" : self.id_to_class (convertir les cl√©s int en string)
2. Retourner ce dictionnaire

**Pourquoi convertir les cl√©s** : JSON n'accepte que des cl√©s string.

---

#### **M√©thode : from_json_dict(data)** (classmethod)

**Ce qu'elle doit faire** :
1. Cr√©er une nouvelle instance de LabelEncoder
2. Charger `class_to_id` depuis data
3. Charger `id_to_class` depuis data (convertir les cl√©s string en int)
4. Retourner l'instance

---

### **Classe 4 : ModelFactory**

**Responsabilit√©** : Construire l'architecture du mod√®le CNN.

---

#### **M√©thode : build(cfg)**

**Ce qu'elle doit faire** :

**Param√®tres** :
- `cfg` : ModelConfig

---

**√âtape 1 : Charger le backbone pr√©-entra√Æn√©**
- Utiliser `keras.applications.MobileNetV2` (ou EfficientNet, ResNet, etc.)
- Param√®tres :
  - `input_shape` : (*cfg.img_size, 3)
  - `include_top` : False (on ne veut pas la couche de classification)
  - `weights` : 'imagenet' (poids pr√©-entra√Æn√©s)
- Figer les poids : `backbone.trainable = False`

**Pourquoi figer** : Transfer learning - on utilise les features apprises sur ImageNet.

---

**√âtape 2 : Construire le mod√®le complet**

**Sous-√©tape 2.1 : Cr√©er l'input**
- `inputs = layers.Input(shape=(*cfg.img_size, 3))`

**Sous-√©tape 2.2 : Passer par le backbone**
- `x = backbone(inputs, training=False)`
- R√©sultat : features maps (7, 7, 1280) pour MobileNetV2

**Sous-√©tape 2.3 : Global Average Pooling**
- `x = layers.GlobalAveragePooling2D()(x)`
- R√©sultat : vecteur (1280,)

**Pourquoi GAP** : R√©duit les dimensions spatiales en une se
ule valeur par channel.

**Sous-√©tape 2.4 : Dropout**
- `x = layers.Dropout(0.2)(x)`
- R√©sultat : vecteur (1280,) avec dropout

**Pourquoi Dropout** : R√©gularisation pour √©viter l'overfitting.

**Sous-√©tape 2.5 : Couche de classification**
- `outputs = layers.Dense(cfg.num_classes, activation='softmax')(x)`
- R√©sultat : vecteur (num_classes,) avec probabilit√©s

**Pourquoi softmax** : Transforme les logits en probabilit√©s qui somment √† 1.

---

**√âtape 3 : Cr√©er le mod√®le Keras**
- `model = keras.Model(inputs, outputs)`

---

**√âtape 4 : Compiler le mod√®le**
- Optimizer : Adam avec learning rate de cfg
- Loss : sparse_categorical_crossentropy (labels sont des entiers)
- Metrics : accuracy

---

**√âtape 5 : Retourner le mod√®le**

---

**Architecture compl√®te** :
```
Input (224, 224, 3)
    ‚Üì
MobileNetV2 (frozen)
    ‚Üì
GlobalAveragePooling2D
    ‚Üì
Dropout(0.2)
    ‚Üì
Dense(num_classes, softmax)
```

---

### **Classe 5 : ModelBundle**

**Responsabilit√©** : Encapsuler tout ce qui est n√©cessaire pour sauvegarder et charger un mod√®le.

---

#### **M√©thode : __init__(model, labels, cfg, preprocess, paths)**

**Ce qu'elle doit faire** :
1. Recevoir un mod√®le Keras
2. Recevoir un LabelEncoder
3. Recevoir une ModelConfig
4. Recevoir un dictionnaire preprocess (optionnel)
5. Recevoir un ModelPaths (optionnel, cr√©er par d√©faut)
6. Stocker tous ces objets comme attributs

---

#### **M√©thode : save(out_dir)**

**Ce qu'elle doit faire** :

**Param√®tres** :
- `out_dir` : Path vers le dossier de sortie

---

**√âtape 1 : Cr√©er le dossier de sortie**
- S'assurer que out_dir existe

---

**√âtape 2 : Sauvegarder le mod√®le**
- Utiliser `model.save(out_dir / self.paths.model_file)`
- Format : .keras (nouveau format Keras 3)

---

**√âtape 3 : Sauvegarder les labels**
- Convertir labels en dictionnaire avec `labels.to_json_dict()`
- √âcrire en JSON dans out_dir / self.paths.labels_file

---

**√âtape 4 : Sauvegarder la config**
- Convertir cfg en dictionnaire
- √âcrire en JSON dans out_dir / self.paths.config_file

---

**√âtape 5 : Sauvegarder preprocess**
- √âcrire self.preprocess en JSON dans out_dir / self.paths.preprocess_file

---

#### **M√©thode : load(in_dir)** (classmethod)

**Ce qu'elle doit faire** :

**Param√®tres** :
- `in_dir` : Path vers le dossier contenant les fichiers

---

**√âtape 1 : Charger le mod√®le**
- Utiliser `keras.models.load_model(in_dir / "model.keras")`

---

**√âtape 2 : Charger les labels**
- Lire le JSON
- Cr√©er un LabelEncoder avec `LabelEncoder.from_json_dict(data)`

---

**√âtape 3 : Charger la config**
- Lire le JSON
- Cr√©er un ModelConfig √† partir du dictionnaire

---

**√âtape 4 : Charger preprocess**
- Lire le JSON

---

**√âtape 5 : Cr√©er et retourner un ModelBundle**
- Avec tous les objets charg√©s

---

#### **M√©thode : load_from_zip(zip_path, extract_dir)** (classmethod)

**Ce qu'elle doit faire** :
1. Extraire le ZIP dans extract_dir
2. Appeler `load(extract_dir)`
3. Retourner le ModelBundle

---

<a id="train-pipeline"></a>
## 8. leaffliction/train_pipeline.py ‚Äî Pipeline d'entra√Ænement

### **Classe 1 : TrainConfig (dataclass)**

**Responsabilit√©** : Stocker tous les hyperparam√®tres d'entra√Ænement.

---

#### **Attributs**

**Ce qu'elle doit contenir** :
- `epochs` : Entier, nombre d'epochs
- `batch_size` : Entier, taille des batchs
- `lr` : Float, learning rate
- `valid_ratio` : Float, ratio de validation (ex: 0.2)
- `seed` : Entier, seed pour reproductibilit√©
- `img_size` : Tuple (H, W)
- `augment_in_train` : Bool√©en, activer les augmentations
- `export_increased_images` : Bool√©en, exporter les images augment√©es
- `extra` : Dictionnaire pour param√®tres additionnels

---

### **Classe 2 : Metrics (dataclass)**

**Responsabilit√©** : Stocker les m√©triques d'entra√Ænement.

---

#### **Attributs**

**Ce qu'elle doit contenir** :
- `train_accuracy` : Float, accuracy sur train
- `valid_accuracy` : Float, accuracy sur validation
- `valid_count` : Entier, nombre d'images de validation
- `notes` : Dictionnaire pour informations additionnelles

---

### **Classe 3 : Trainer**

**Responsabilit√©** : Orchestrer tout le processus d'entra√Ænement.

---

#### **M√©thode : __init__(dataset_scanner, dataset_splitter, model_factory, labels)**

**Ce qu'elle doit faire** :
1. Recevoir un DatasetScanner
2. Recevoir un DatasetSplitter
3. Recevoir un ModelFactory
4. Recevoir un LabelEncoder
5. Stocker comme attributs

---

#### **M√©thode : train(dataset_dir, out_dir, cfg)**

**Ce qu'elle doit faire** :

**Param√®tres** :
- `dataset_dir` : Path vers le dataset
- `out_dir` : Path vers le dossier de sortie
- `cfg` : TrainConfig

---

**√âtape 1 : Scanner le dataset**
- Utiliser `dataset_scanner.ft_scan(dataset_dir)`
- R√©sultat : DatasetIndex

---

**√âtape 2 : Fitter le LabelEncoder**
- Utiliser `labels.fit(index.class_names)`

---

**√âtape 3 : Splitter train/valid**
- Utiliser `dataset_splitter.ft_split(index.items, cfg.valid_ratio, cfg.seed, stratified=True)`
- R√©sultat : (train_items, valid_items)

---

**√âtape 4 : Cr√©er l'augmenteur (si demand√©)**
- Si cfg.augment_in_train :
  - Cr√©er un KerasAugmentationsFactory
  - Appeler `factory.build()`
  - R√©sultat : keras.Sequential d'augmentations
- Sinon : None

---

**√âtape 5 : Construire les tf.data.Dataset**
- Cr√©er une TFDataConfig avec les param√®tres de cfg
- Cr√©er un TFDatasetBuilder avec la config et l'augmenteur
- Construire train_ds : `builder.build(train_items, training=True)`
- Construire valid_ds : `builder.build(valid_items, training=False)`

---

**√âtape 6 : Construire le mod√®le**
- Cr√©er une ModelConfig avec img_size et num_classes
- Utiliser `model_factory.build(model_cfg)`
- R√©sultat : mod√®le Keras compil√©

---

**√âtape 7 : Cr√©er les callbacks**
- Utiliser KerasCallbacksFactory pour cr√©er :
  - EarlyStopping
  - ModelCheckpoint
  - ReduceLROnPlateau
  - TensorBoard (optionnel)

---

**√âtape 8 : Entra√Æner le mod√®le**
- Appeler `model.fit(train_ds, validation_data=valid_ds, epochs=cfg.epochs, callbacks=callbacks)`
- R√©sultat : history

---

**√âtape 9 : √âvaluer sur validation**
- Appeler `model.evaluate(valid_ds)`
- Extraire la loss et l'accuracy

---

**√âtape 10 : Cr√©er les m√©triques**
- Cr√©er un objet Metrics avec :
  - train_accuracy : depuis history
  - valid_accuracy : depuis evaluate
  - valid_count : len(valid_items)

---

**√âtape 11 : Sauvegarder le ModelBundle**
- Cr√©er un ModelBundle avec le mod√®le, labels, config
- Appeler `bundle.save(out_dir / "model")`

---

**√âtape 12 : Exporter les images augment√©es (si demand√©)**
- Si cfg.export_increased_images :
  - Pour un √©chantillon d'images :
    - Appliquer les augmentations
    - Sauvegarder dans out_dir / "augmented"

---

**√âtape 13 : Retourner les m√©triques**

---

### **Classe 4 : RequirementsGate**

**Responsabilit√©** : Valider que les contraintes du sujet sont respect√©es.

---

#### **M√©thode : assert_ok(metrics)**

**Ce qu'elle doit faire** :

**Param√®tres** :
- `metrics` : Objet Metrics

---

**V√©rification 1 : Accuracy > 90%**
- Si metrics.valid_accuracy < 0.90 :
  - Lever une ValueError avec un message clair
  - Exemple : "Validation accuracy 87.5% < 90%. Training failed."

---

**V√©rification 2 : Validation set >= 100 images**
- Si metrics.valid_count < 100 :
  - Lever une ValueError
  - Exemple : "Validation set has 85 images < 100."

---

**Si tout est OK**
- Afficher un message de succ√®s
- Retourner (ou ne rien faire)

---

### **Classe 5 : TrainingPackager**

**Responsabilit√©** : Pr√©parer les artefacts et cr√©er le ZIP final.

---

#### **M√©thode : __init__(zip_packager)**

**Ce qu'elle doit faire** :
1. Recevoir un ZipPackager
2. Stocker comme attribut

---

#### **M√©thode : prepare_artifacts_dir(tmp_dir)**

**Ce qu'elle doit faire** :

**Param√®tres** :
- `tmp_dir` : Path vers le dossier temporaire

---

**√âtape 1 : Cr√©er un dossier artifacts**
- Cr√©er tmp_dir / "artifacts" s'il n'existe pas

---

**√âtape 2 : Copier les fichiers n√©cessaires**
- Copier model/ vers artifacts/model/
- Copier augmented/ vers artifacts/augmented/ (si existe)

---

**√âtape 3 : Retourner le chemin vers artifacts**

---

#### **M√©thode : build_zip(artifacts_dir, out_zip)**

**Ce qu'elle doit faire** :
1. Utiliser `zip_packager.ft_zip_dir(artifacts_dir, out_zip)`
2. Afficher un message de succ√®s

---

### **Classe 6 : KerasCallbacksFactory**

**Responsabilit√©** : Cr√©er les callbacks Keras pour am√©liorer l'entra√Ænement.

---

#### **M√©thode : build(out_dir)**

**Ce qu'elle doit faire** :

**Param√®tres** :
- `out_dir` : Path pour sauvegarder les checkpoints

---

**Callback 1 : EarlyStopping**
- Monitor : 'val_accuracy'
- Patience : 5 epochs
- Restore best weights : True
- Verbose : 1

**Effet** : Arr√™te l'entra√Ænement si pas d'am√©lioration pendant 5 epochs.

---

**Callback 2 : ModelCheckpoint**
- Filepath : out_dir / "best_model.keras"
- Monitor : 'val_accuracy'
- Save best only : True
- Verbose : 1

**Effet** : Sauvegarde le meilleur mod√®le automatiquement.

---

**Callback 3 : ReduceLROnPlateau**
- Monitor : 'val_loss'
- Factor : 0.5 (divise le LR par 2)
- Patience : 3 epochs
- Min LR : 1e-7
- Verbose : 1

**Effet** : R√©duit le learning rate si plateau d√©tect√©.

---

**Callback 4 : TensorBoard (optionnel)**
- Log dir : out_dir / "tensorboard"

**Effet** : Permet de visualiser l'entra√Ænement avec TensorBoard.

---

**Retourner la liste des callbacks**

---

<a id="predict-pipeline"></a>
## 9. leaffliction/predict_pipeline.py ‚Äî Pipeline de pr√©diction

### **Classe 1 : PredictConfig (dataclass)**

**Responsabilit√©** : Stocker la configuration pour la pr√©diction.

---

#### **Attributs**

**Ce qu'elle doit contenir** :
- `show_transforms` : Bool√©en, afficher les transformations
- `top_k` : Entier, nombre de pr√©dictions √† afficher
- `extra` : Dictionnaire pour param√®tres additionnels

---

### **Classe 2 : Predictor**

**Responsabilit√©** : Charger le mod√®le et pr√©dire sur une image.

---

#### **M√©thode : __init__(bundle_loader, transformations_engine)**

**Ce qu'elle doit faire** :
1. Recevoir une classe bundle_loader (ModelBundle)
2. Recevoir un TransformationEngine
3. Stocker comme attributs

---

#### **M√©thode : predict(bundle_zip, image_path, cfg)**

**Ce qu'elle doit faire** :

**Param√®tres** :
- `bundle_zip` : Path vers learnings.zip
- `image_path` : Path vers l'image √† pr√©dire
- `cfg` : PredictConfig

---

**√âtape 1 : Charger le ModelBundle**
- Cr√©er un dossier temporaire pour extraction
- Utiliser `bundle_loader.load_from_zip(bundle_zip, temp_dir)`
- R√©sultat : bundle avec mod√®le, labels, config

---

**√âtape 2 : Charger l'image**
- Lire le fichier avec `tf.io.read_file`
- D√©coder avec `tf.image.decode_jpeg`
- Resize √† bundle.cfg.img_size
- Normaliser [0, 1]

---

**√âtape 3 : Appliquer les transformations (si demand√©)**
- Si cfg.show_transforms :
  - Utiliser `transformations_engine.apply_all(img)`
  - Stocker les r√©sultats pour affichage

---

**√âtape 4 : Pr√©parer l'image pour pr√©diction**
- Ajouter une dimension batch : `img_batch = tf.expand_dims(img, 0)`
- R√©sultat : (1, H, W, 3)

---

**√âtape 5 : Pr√©dire**
- Appeler `bundle.model.predict(img_batch)`
- R√©sultat : probabilit√©s (1, num_classes)

---

**√âtape 6 : Extraire les probabilit√©s**
- Squeeze pour enlever la dimension batch : `probs = predictions[0]`

---

**√âtape 7 : Trouver la classe pr√©dite**
- Trouver l'index du maximum : `class_id = np.argmax(probs)`
- D√©coder avec `bundle.labels.decode(class_id)`
- R√©sultat : nom de la classe

---

**√âtape 8 : Cr√©er le dictionnaire de probabilit√©s**
- Pour chaque classe :
  - Cr√©er {nom_classe: probabilit√©}
- Trier par probabilit√© d√©croissante
- Garder seulement les top_k

---

**√âtape 9 : Retourner**
- Retourner (predicted_label, probs_dict)

---

### **Classe 3 : PredictionVisualiser**

**Responsabilit√©** : Afficher visuellement le r√©sultat de la pr√©diction.

---

#### **M√©thode : show(original, transformed, predicted_label)**

**Ce qu'elle doit faire** :

**Param√®tres** :
- `original` : Image originale
- `transformed` : Dictionnaire {nom: image_transform√©e}
- `predicted_label` : String, classe pr√©dite

---

**√âtape 1 : Calculer le layout**
- Nombre total d'images : 1 (original) + len(transformed)
- Calculer rows et cols

---

**√âtape 2 : Cr√©er la figure**
- Cr√©er une grille de subplots

---

**√âtape 3 : Afficher l'original**
- Titre : f"Original - Predicted: {predicted_label}"
- Utiliser un cadre vert pour indiquer la pr√©diction

---

**√âtape 4 : Afficher les transformations**
- Pour chaque transformation :
  - Afficher avec son nom comme titre

---

**√âtape 5 : Ajuster et afficher**
- tight_layout()
- show()

---

**Pourquoi afficher les transformations** : Permet de voir comment le mod√®le "voit" l'image apr√®s diff√©rentes transformations, utile pour le debugging.

---

## üéØ Conclusion du guide d'impl√©mentation

### Ce que vous avez maintenant

**Un guide complet qui explique** :
1. ‚úÖ **Quoi faire** : Chaque classe et m√©thode est d√©crite
2. ‚úÖ **Pourquoi le faire** : Les justifications sont donn√©es
3. ‚úÖ **Comment le faire** : Les √©tapes sont d√©taill√©es
4. ‚úÖ **Sans code** : Vous pouvez impl√©menter vous-m√™me

### Comment utiliser ce guide

**Pour chaque fichier √† impl√©menter** :
1. Lire la section correspondante dans ce guide
2. Comprendre la responsabilit√© de chaque classe
3. Suivre les √©tapes d√©crites pour chaque m√©thode
4. Impl√©menter dans votre propre style
5. Tester votre impl√©mentation

### Ordre d'impl√©mentation recommand√©

**Phase 1 : Fondations**
1. `utils.py` (PathManager, Hasher, ZipPackager)
2. `cli.py` (tous les parsers)

**Phase 2 : Dataset**
3. `dataset.py` (DatasetIndex, Scanner, Splitter, TFDatasetBuilder)
4. `plotting.py` (DistributionPlotter, GridPlotter)

**Phase 3 : Transformations**
5. `augmentations.py` (6 augmentations + Engine + Saver)
6. `transformations.py` (6 transformations + Engine + BatchTransformer)

**Phase 4 : Mod√®le**
7. `model.py` (LabelEncoder, ModelFactory, ModelBundle)

**Phase 5 : Pipelines**
8. `train_pipeline.py` (Trainer, RequirementsGate, Packager)
9. `predict_pipeline.py` (Predictor, Visualiser)

**Phase 6 : Scripts racine**
10. `Distribution.py`
11. `Augmentation.py`
12. `Transformation.py`
13. `train.py`
14. `predict.py`

### Points critiques √† ne pas oublier

**Reproductibilit√©** :
- Toujours fixer les seeds (numpy, tensorflow, random)
- Utiliser le m√™me ordre de tri partout

**Validation** :
- Split stratifi√© obligatoire
- Accuracy > 90% obligatoire
- Validation set >= 100 images obligatoire

**Sauvegarde** :
- Chemins relatifs dans les ZIP
- SHA1 correct pour signature.txt
- Tous les fichiers n√©cessaires dans le bundle

**Performance** :
- Utiliser AUTOTUNE pour tf.data
- Prefetch pour ne pas bloquer le GPU
- Cache si le dataset tient en RAM

### Ressources compl√©mentaires

**Documentation officielle** :
- TensorFlow : https://www.tensorflow.org/api_docs
- Keras : https://keras.io/api/
- OpenCV : https://docs.opencv.org/

**Pour la soutenance** :
- R√©f√©rez-vous au GUIDE.md pour les formules math√©matiques
- Pr√©parez des exemples de r√©sultats
- Soyez capable d'expliquer chaque choix d'architecture

---

**Bon courage pour l'impl√©mentation ! üöÄ**

---

> **Note finale** : Ce guide est votre feuille de route. Chaque section est con√ßue pour que vous puissiez impl√©menter le code vous-m√™me en comprenant exactement ce que chaque partie doit faire. Si vous suivez ce guide √©tape par √©tape, vous aurez un projet Leaffliction complet et fonctionnel.
