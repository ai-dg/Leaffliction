# üìñ Guide Conceptuel d'Impl√©mentation ‚Äî Leaffliction (PyTorch)

> **Objectif** : Expliquer **litt√©ralement** ce que chaque classe doit faire, **sans code**, pour une approche **PyTorch avec transformations comme features**.

---

## üìë Table des mati√®res

1. [Vue d'ensemble : PyTorch avec Transformations](#vue-densemble)
2. [Pipeline PyTorch](#pipeline-pytorch)
3. [leaffliction/cli.py ‚Äî Parsers d'arguments](#cli)
4. [leaffliction/utils.py ‚Äî Utilitaires](#utils)
5. [leaffliction/dataset.py ‚Äî Gestion du dataset](#dataset)
6. [leaffliction/plotting.py ‚Äî Visualisations](#plotting)
7. [leaffliction/augmentations.py ‚Äî Augmentations](#augmentations)
8. [leaffliction/transformations.py ‚Äî Cr√©ation de Tensors](#transformations)
9. [leaffliction/model.py ‚Äî Mod√®le PyTorch](#model)
10. [leaffliction/train_pipeline.py ‚Äî Pipeline d'entra√Ænement](#train-pipeline)
11. [leaffliction/predict_pipeline.py ‚Äî Pipeline de pr√©diction](#predict-pipeline)

---

<a id="vue-densemble"></a>
## 1. Vue d'ensemble : PyTorch avec Transformations

### Architecture Unique

Cette approche combine :
- ‚úÖ **Transformations manuelles** (Grayscale, Canny, etc.) comme **canaux d'entr√©e**
- ‚úÖ **CNN simple** (PyTorch) pour apprendre des patterns sur ces transformations
- ‚úÖ **Pas de features manuelles** (histogrammes, stats)

### Concept Cl√©

```
Image RGB (H, W, 3)
     ‚Üì
Appliquer 6 transformations
     ‚Üì
Cr√©er tensor (6, H, W)  ‚Üê 6 canaux au lieu de 3 RGB
     ‚Üì
CNN PyTorch
     ‚Üì
Classification
```

### Avantages

‚úÖ **Plus performant** que features manuelles (histogrammes)
‚úÖ **Plus simple** qu'un CNN complet (pas besoin de millions d'images)
‚úÖ **Interpr√©table** : On sait quels canaux sont utilis√©s
‚úÖ **Rapide** : Entra√Ænement en quelques minutes
‚úÖ **Flexible** : Architecture PyTorch modifiable

---

<a id="pipeline-pytorch"></a>
## 2. Pipeline PyTorch

### Sch√©ma Complet

```
(1) Dataset brut
     ‚Üì
(2) Scan + Split train/valid
     ‚Üì
(3) Augmentation du train set (images physiques)
     ‚Üì Cr√©e plus d'images sur disque
     ‚Üì
(4) Transformation en tensors PyTorch
     ‚Üì Applique 6 transformations par image
     ‚Üì Cr√©e tensor (n, 6, H, W)
     ‚Üì
(5) Cr√©ation DataLoaders PyTorch
     ‚Üì
(6) Entra√Ænement CNN (backpropagation)
     ‚Üì
(7) √âvaluation (accuracy > 90%)
     ‚Üì
(8) Sauvegarde (model.pth, labels.json)
     ‚Üì
(9) Packaging (learnings.zip)
```

### Les 6 Transformations comme Canaux

```python
Canal 0: Grayscale
Canal 1: Canny Edges
Canal 2: Histogram Equalisation
Canal 3: Sharpen
Canal 4: Threshold
Canal 5: Morphology

Tensor final: (batch, 6, 224, 224)
```

---

<a id="cli"></a>
## 3. leaffliction/cli.py ‚Äî Parsers d'arguments

**Statut** : ‚úÖ D√©j√† impl√©ment√©, pas de changement n√©cessaire.

---

<a id="utils"></a>
## 4. leaffliction/utils.py ‚Äî Utilitaires

**Statut** : ‚úÖ D√©j√† impl√©ment√©, pas de changement n√©cessaire.

---

<a id="dataset"></a>
## 5. leaffliction/dataset.py ‚Äî Gestion du dataset

### Changements par rapport √† ML traditionnel

**√Ä SUPPRIMER** :
- ‚ùå `TFDataConfig` (pas besoin de tf.data)
- ‚ùå `TFDatasetBuilder` (pas besoin de pipeline TensorFlow)

**√Ä GARDER** :
- ‚úÖ `DatasetIndex`
- ‚úÖ `DatasetScanner`
- ‚úÖ `DatasetSplitter`

---

### **Classe : DatasetScanner**

**M√©thode : scan(root)**

**Ce qu'elle doit faire** :

**√âtape 1 : Lister les sous-dossiers**
- Recevoir un chemin vers le dossier racine (Path)
- Lister TOUS les sous-dossiers directs
- Filtrer pour ne garder que les dossiers (pas les fichiers)
- Trier alphab√©tiquement

**√âtape 2 : Extraire les noms de classes**
- Pour chaque dossier, extraire son nom
- Ces noms deviennent `class_names`
- L'ordre d√©termine les `class_id` (0, 1, 2, ...)

**√âtape 3 : Scanner chaque classe**
- Pour chaque dossier (avec son index comme class_id) :
  - Utiliser `PathManager.ft_iter_images()` pour lister les images
  - Compter le nombre d'images
  - Pour chaque image :
    - Cr√©er un tuple `(chemin_image, class_id)`
    - Ajouter √† la liste `items`
  - Stocker le compte dans `counts`

**√âtape 4 : Retourner**
- Cr√©er un `DatasetIndex` avec toutes ces informations
- Retourner cet objet

---

### **Classe : DatasetSplitter**

**Statut** : ‚úÖ D√©j√† impl√©ment√© (split stratifi√©)

Pas de changement n√©cessaire.

---

<a id="plotting"></a>
## 6. leaffliction/plotting.py ‚Äî Visualisations

**Statut** : ‚úÖ D√©j√† impl√©ment√©, pas de changement n√©cessaire.

---

<a id="augmentations"></a>
## 7. leaffliction/augmentations.py ‚Äî Augmentations

### R√¥le dans PyTorch

**Augmentations** = Cr√©er des **images physiques** sur disque (TRAIN ONLY)

**Diff√©rence avec CNN classique** :
- **CNN classique** : Augmentations √† la vol√©e (dans le DataLoader)
- **Notre approche** : Augmentations cr√©ent des fichiers AVANT le training

**Pourquoi** : Simplifie le pipeline et permet de visualiser les augmentations.

---

### **Classe : AugmentationEngine**

#### **M√©thode : augment_dataset(train_items, output_dir, augmentations_per_image)**

**Ce qu'elle doit faire** :

**Param√®tres** :
- `train_items` : Liste de tuples `(Path, class_id)`
- `output_dir` : Dossier o√π sauvegarder les images augment√©es
- `augmentations_per_image` : Nombre d'augmentations par image (ex: 3)

**Retour** :
- Liste √©tendue : originales + augment√©es

---

**√âtape 1 : Cr√©er le dossier de sortie**
- S'assurer que `output_dir` existe
- Cr√©er les sous-dossiers par classe si n√©cessaire

**√âtape 2 : Initialiser la liste de retour**
- Cr√©er une liste vide `augmented_items`

**√âtape 3 : Pour chaque image du train set**

**Sous-√©tape 3.1 : Garder l'originale**
- Ajouter `(img_path, label)` √† `augmented_items`

**Sous-√©tape 3.2 : Charger l'image**
- Utiliser OpenCV : `cv2.imread(str(img_path))`
- Convertir BGR ‚Üí RGB : `cv2.cvtColor(img, cv2.COLOR_BGR2RGB)`

**Sous-√©tape 3.3 : Cr√©er N versions augment√©es**
- Pour `i` de 0 √† `augmentations_per_image - 1` :
  - Appliquer `apply_random(img, n=2)` (2 augmentations al√©atoires)
  - G√©n√©rer un nom de fichier : `{stem}_aug{i}{ext}`
  - Cr√©er le chemin complet : `output_dir / class_name / filename`
  - Cr√©er les dossiers parents si n√©cessaire
  - Convertir RGB ‚Üí BGR
  - Sauvegarder avec `cv2.imwrite()`
  - Ajouter `(aug_path, label)` √† `augmented_items`

**√âtape 4 : Retourner**
- Retourner `augmented_items` (liste √©tendue)

---

### **Les 6 Augmentations**

Toutes travaillent avec **NumPy arrays** et **OpenCV**.

#### **FlipHorizontalAug**
```python
def apply(self, img: np.ndarray) -> np.ndarray:
    return cv2.flip(img, 1)  # 1 = horizontal
```

#### **FlipVerticalAug**
```python
def apply(self, img: np.ndarray) -> np.ndarray:
    return cv2.flip(img, 0)  # 0 = vertical
```

#### **RotateAug**
```python
def apply(self, img: np.ndarray) -> np.ndarray:
    h, w = img.shape[:2]
    center = (w // 2, h // 2)
    M = cv2.getRotationMatrix2D(center, self.angle, 1.0)
    return cv2.warpAffine(img, M, (w, h))
```

#### **BrightnessContrastAug**
```python
def apply(self, img: np.ndarray) -> np.ndarray:
    img = img.astype(np.float32)
    img = img * (1 + self.contrast) + self.brightness
    img = np.clip(img, 0, 255)
    return img.astype(np.uint8)
```

#### **GaussianBlurAug**
```python
def apply(self, img: np.ndarray) -> np.ndarray:
    ksize = int(2 * np.ceil(3 * self.sigma) + 1)
    if ksize % 2 == 0:
        ksize += 1
    return cv2.GaussianBlur(img, (ksize, ksize), self.sigma)
```

#### **RandomCropResizeAug**
```python
def apply(self, img: np.ndarray) -> np.ndarray:
    h, w = img.shape[:2]
    new_h = int(h * self.crop_ratio)
    new_w = int(w * self.crop_ratio)
    
    top = random.randint(0, h - new_h)
    left = random.randint(0, w - new_w)
    
    cropped = img[top:top+new_h, left:left+new_w]
    return cv2.resize(cropped, (w, h))
```

---

<a id="transformations"></a>
## 8. leaffliction/transformations.py ‚Äî Cr√©ation de Tensors

### R√¥le dans PyTorch

**Transformations** = **Cr√©ation de canaux** pour le CNN

Les transformations ne sont plus pour extraire des features num√©riques, mais pour cr√©er des **canaux visuels** que le CNN va analyser.

---

### **Classe : TransformationEngine**

#### **M√©thode : apply_all_as_tensor(img)**

**Ce qu'elle doit faire** :

**Param√®tres** :
- `img` : Image RGB (H, W, 3) en NumPy array

**Retour** :
- `torch.Tensor` de shape `(n_transforms, H, W)`

---

**√âtape 1 : Initialiser la liste de canaux**
- Cr√©er une liste vide `channels = []`

**√âtape 2 : Pour chaque transformation**
- Appliquer la transformation : `transformed = tf.apply(img)`
- Si l'image est en couleur (3 canaux) :
  - Convertir en grayscale : `cv2.cvtColor(transformed, cv2.COLOR_RGB2GRAY)`
- Normaliser [0, 255] ‚Üí [0, 1] :
  - `transformed = transformed.astype(np.float32) / 255.0`
- Ajouter √† la liste : `channels.append(transformed)`

**√âtape 3 : Stack en tensor**
- Utiliser NumPy : `stacked = np.stack(channels, axis=0)`
- Convertir en PyTorch : `tensor = torch.from_numpy(stacked)`
- Shape finale : `(n_transforms, H, W)`

**√âtape 4 : Retourner**
- Retourner le tensor

---

#### **M√©thode : batch_transform(items, img_size)**

**Ce qu'elle doit faire** :

**Param√®tres** :
- `items` : Liste de tuples `(Path, class_id)`
- `img_size` : Tuple `(H, W)` pour redimensionner (ex: (224, 224))

**Retour** :
- `X` : torch.Tensor de shape `(n, n_transforms, H, W)`
- `y` : torch.Tensor de shape `(n,)`

---

**√âtape 1 : Initialiser les listes**
- Cr√©er `X_list = []` et `y_list = []`

**√âtape 2 : Pour chaque item**

**Sous-√©tape 2.1 : Charger l'image**
- Utiliser OpenCV : `img = cv2.imread(str(img_path))`
- V√©rifier si l'image est charg√©e (pas None)
- Convertir BGR ‚Üí RGB : `cv2.cvtColor(img, cv2.COLOR_BGR2RGB)`
- Redimensionner : `cv2.resize(img, img_size)`

**Sous-√©tape 2.2 : Transformer en tensor**
- Appeler `apply_all_as_tensor(img)`
- Ajouter √† X_list : `X_list.append(tensor)`
- Ajouter le label √† y_list : `y_list.append(label)`

**√âtape 3 : Stack en batch**
- `X = torch.stack(X_list)` ‚Üí shape `(n, n_transforms, H, W)`
- `y = torch.tensor(y_list, dtype=torch.long)` ‚Üí shape `(n,)`

**√âtape 4 : Retourner**
- Retourner `(X, y)`

---

### **Les 6 Transformations**

Identiques aux augmentations, mais appliqu√©es pour cr√©er des canaux.

#### **GrayscaleTf**
```python
def apply(self, img: np.ndarray) -> np.ndarray:
    return cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
```

#### **CannyEdgesTf**
```python
def apply(self, img: np.ndarray) -> np.ndarray:
    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
    return cv2.Canny(gray, self.threshold1, self.threshold2)
```

#### **HistogramEqualisationTf**
```python
def apply(self, img: np.ndarray) -> np.ndarray:
    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
    return cv2.equalizeHist(gray)
```

#### **SharpenTf**
```python
def apply(self, img: np.ndarray) -> np.ndarray:
    kernel = np.array([[-1,-1,-1],
                       [-1, 9,-1],
                       [-1,-1,-1]])
    return cv2.filter2D(img, -1, kernel)
```

#### **ThresholdTf**
```python
def apply(self, img: np.ndarray) -> np.ndarray:
    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
    _, thresh = cv2.threshold(gray, self.threshold, 255, cv2.THRESH_BINARY)
    return thresh
```

#### **MorphologyTf**
```python
def apply(self, img: np.ndarray) -> np.ndarray:
    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
    kernel = np.ones((self.kernel_size, self.kernel_size), np.uint8)
    
    if self.mode == "erode":
        return cv2.erode(gray, kernel)
    elif self.mode == "dilate":
        return cv2.dilate(gray, kernel)
    # etc.
```

---

<a id="model"></a>
## 9. leaffliction/model.py ‚Äî Mod√®le PyTorch

### Architecture : TransformationClassifier

**Input** : `(batch, 6, 224, 224)` - 6 canaux de transformations

**Output** : `(batch, num_classes)` - Logits pour chaque classe

---

### **Classe : TransformationClassifier (nn.Module)**

#### **Architecture**

```
Input: (batch, 6, 224, 224)
     ‚Üì
Conv2D(6‚Üí32) + ReLU + MaxPool(2)
     ‚Üí (batch, 32, 112, 112)
     ‚Üì
Conv2D(32‚Üí64) + ReLU + MaxPool(2)
     ‚Üí (batch, 64, 56, 56)
     ‚Üì
Conv2D(64‚Üí128) + ReLU + MaxPool(2)
     ‚Üí (batch, 128, 28, 28)
     ‚Üì
Conv2D(128‚Üí256) + ReLU + MaxPool(2)
     ‚Üí (batch, 256, 14, 14)
     ‚Üì
GlobalAveragePooling
     ‚Üí (batch, 256, 1, 1)
     ‚Üì
Flatten
     ‚Üí (batch, 256)
     ‚Üì
Dense(256‚Üí128) + ReLU + Dropout(0.5)
     ‚Üí (batch, 128)
     ‚Üì
Dense(128‚Üínum_classes)
     ‚Üí (batch, num_classes)
```

---

#### **M√©thode : __init__(num_classes, input_channels)**

**Ce qu'elle doit faire** :

**Param√®tres** :
- `num_classes` : Nombre de classes (ex: 7)
- `input_channels` : Nombre de transformations (ex: 6)

---

**√âtape 1 : D√©finir les convolutions**
- Cr√©er un `nn.Sequential` avec :
  - Conv2D(input_channels ‚Üí 32, kernel=3, padding=1)
  - ReLU
  - MaxPool2D(2)
  - Conv2D(32 ‚Üí 64, kernel=3, padding=1)
  - ReLU
  - MaxPool2D(2)
  - Conv2D(64 ‚Üí 128, kernel=3, padding=1)
  - ReLU
  - MaxPool2D(2)
  - Conv2D(128 ‚Üí 256, kernel=3, padding=1)
  - ReLU
  - MaxPool2D(2)

**√âtape 2 : D√©finir le Global Average Pooling**
- `nn.AdaptiveAvgPool2d(1)` - R√©duit √† (batch, 256, 1, 1)

**√âtape 3 : D√©finir le classifier**
- Cr√©er un `nn.Sequential` avec :
  - Flatten
  - Linear(256 ‚Üí 128)
  - ReLU
  - Dropout(0.5)
  - Linear(128 ‚Üí num_classes)

---

#### **M√©thode : forward(x)**

**Ce qu'elle doit faire** :

**Param√®tres** :
- `x` : Tensor de shape `(batch, 6, 224, 224)`

**Retour** :
- Tensor de shape `(batch, num_classes)`

---

**√âtape 1 : Passer par les convolutions**
- `x = self.features(x)` ‚Üí `(batch, 256, 14, 14)`

**√âtape 2 : Global Average Pooling**
- `x = self.gap(x)` ‚Üí `(batch, 256, 1, 1)`

**√âtape 3 : Classifier**
- `x = self.classifier(x)` ‚Üí `(batch, num_classes)`

**√âtape 4 : Retourner**
- Retourner `x` (logits)

---

### **Classe : PyTorchModelFactory**

#### **M√©thode : build(cfg)**

**Ce qu'elle doit faire** :

**Param√®tres** :
- `cfg` : ModelConfig

**Retour** :
- `TransformationClassifier` non entra√Æn√©

---

**√âtape 1 : Cr√©er le mod√®le**
```python
model = TransformationClassifier(
    num_classes=cfg.num_classes,
    input_channels=cfg.input_channels  # 6
)
```

**√âtape 2 : Retourner**
- Retourner le mod√®le

---

### **Classe : PyTorchModelBundle**

#### **Attributs**

**Ce qu'elle doit contenir** :
- `model` : TransformationClassifier entra√Æn√©
- `labels` : LabelEncoder (mapping classe ‚Üî id)
- `transformation_engine` : TransformationEngine (pour cr√©er tensors)
- `cfg` : ModelConfig
- `device` : torch.device (CPU ou GPU)

---

#### **M√©thode : save(out_dir)**

**Ce qu'elle doit faire** :

**√âtape 1 : Cr√©er le dossier**
- S'assurer que `out_dir` existe

**√âtape 2 : Sauvegarder le mod√®le**
- Utiliser PyTorch : `torch.save(self.model.state_dict(), out_dir / "model.pth")`

**√âtape 3 : Sauvegarder les labels**
- Convertir en dict : `labels_dict = self.labels.to_json_dict()`
- √âcrire en JSON : `json.dump(labels_dict, open(out_dir / "labels.json", "w"))`

**√âtape 4 : Sauvegarder la config**
- Convertir cfg en dict
- √âcrire en JSON : `json.dump(config_dict, open(out_dir / "config.json", "w"))`

---

#### **M√©thode : load(in_dir)** (classmethod)

**Ce qu'elle doit faire** :

**√âtape 1 : Charger la config**
- Lire le JSON
- Cr√©er un ModelConfig

**√âtape 2 : Charger les labels**
- Lire le JSON
- Cr√©er un LabelEncoder : `labels = LabelEncoder.from_json_dict(data)`

**√âtape 3 : Cr√©er le mod√®le**
- `model = TransformationClassifier(cfg.num_classes, cfg.input_channels)`
- Charger les poids : `model.load_state_dict(torch.load(in_dir / "model.pth"))`

**√âtape 4 : Recr√©er le TransformationEngine**
- Cr√©er un TransformationEngine avec les 6 transformations

**√âtape 5 : Cr√©er et retourner le bundle**
- `return PyTorchModelBundle(model, labels, tf_engine, cfg)`

---

#### **M√©thode : predict(tensor)**

**Ce qu'elle doit faire** :

**Param√®tres** :
- `tensor` : torch.Tensor de shape `(n_transforms, H, W)` ou `(1, n_transforms, H, W)`

**Retour** :
- `pred_id` : int (ID de la classe pr√©dite)
- `probs` : Dict[str, float] (probabilit√©s par classe)

---

**√âtape 1 : Pr√©parer le tensor**
- Si shape `(n_transforms, H, W)` : ajouter batch dimension
  - `tensor = tensor.unsqueeze(0)` ‚Üí `(1, n_transforms, H, W)`
- D√©placer sur le device : `tensor = tensor.to(self.device)`

**√âtape 2 : Mode √©valuation**
- `self.model.eval()`

**√âtape 3 : Pr√©dire (sans gradient)**
```python
with torch.no_grad():
    outputs = self.model(tensor)  # (1, num_classes)
    probs_tensor = torch.softmax(outputs, dim=1)  # Probabilit√©s
    pred_id = torch.argmax(probs_tensor, dim=1).item()  # ID pr√©dit
```

**√âtape 4 : Convertir probs en dict**
```python
probs_np = probs_tensor.cpu().numpy()[0]
probs = {
    self.labels.decode(i): float(probs_np[i])
    for i in range(len(probs_np))
}
```

**√âtape 5 : Retourner**
- `return pred_id, probs`

---

<a id="train-pipeline"></a>
## 10. leaffliction/train_pipeline.py ‚Äî Pipeline d'entra√Ænement

### Pipeline PyTorch

```
1. Scanner dataset
2. Split train/valid
3. Augmenter train set (images physiques)
4. Transformer en tensors PyTorch
5. Cr√©er DataLoaders
6. Entra√Æner avec backpropagation
7. √âvaluer
8. Sauvegarder
```

---

### **Classe : PyTorchTrainer**

#### **M√©thode : train(dataset_dir, out_dir, cfg)**

**Ce qu'elle doit faire** :

**Param√®tres** :
- `dataset_dir` : Path vers le dataset
- `out_dir` : Path vers le dossier de sortie
- `cfg` : TrainConfig

**Retour** :
- `Metrics` (train_accuracy, valid_accuracy, valid_count)

---

**√âtape 1 : Scanner le dataset**
```python
index = self.dataset_scanner.scan(dataset_dir)
```

**√âtape 2 : Fitter le LabelEncoder**
```python
self.labels.fit(index.class_names)
```

**√âtape 3 : Split train/valid**
```python
train_items, valid_items = self.dataset_splitter.split(
    index.items,
    cfg.valid_ratio,
    cfg.seed,
    stratified=True
)
```

**√âtape 4 : Augmenter le train set (optionnel)**
```python
if cfg.augment_train:
    train_items = self.augmentation_engine.augment_dataset(
        train_items,
        out_dir / "augmented",
        cfg.augmentations_per_image
    )
```

**√âtape 5 : Transformer en tensors**
```python
X_train, y_train = self.transformation_engine.batch_transform(
    train_items, 
    cfg.img_size
)
X_valid, y_valid = self.transformation_engine.batch_transform(
    valid_items, 
    cfg.img_size
)
```

**R√©sultat** :
- `X_train` : `(n_train, 6, 224, 224)`
- `y_train` : `(n_train,)`
- `X_valid` : `(n_valid, 6, 224, 224)`
- `y_valid` : `(n_valid,)`

**√âtape 6 : Cr√©er DataLoaders**
```python
from torch.utils.data import TensorDataset, DataLoader

train_dataset = TensorDataset(X_train, y_train)
valid_dataset = TensorDataset(X_valid, y_valid)

train_loader = DataLoader(
    train_dataset, 
    batch_size=cfg.batch_size, 
    shuffle=True
)
valid_loader = DataLoader(
    valid_dataset, 
    batch_size=cfg.batch_size, 
    shuffle=False
)
```

**√âtape 7 : Construire le mod√®le**
```python
model = self.model_factory.build(ModelConfig(
    num_classes=index.num_classes,
    input_channels=6,
    img_size=cfg.img_size
))

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)
```

**√âtape 8 : D√©finir loss et optimizer**
```python
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=cfg.lr)
```

**√âtape 9 : Training loop**

Pour chaque epoch :

**Phase Training** :
```python
model.train()
for X_batch, y_batch in train_loader:
    X_batch, y_batch = X_batch.to(device), y_batch.to(device)
    
    # Forward
    optimizer.zero_grad()
    outputs = model(X_batch)
    loss = criterion(outputs, y_batch)
    
    # Backward
    loss.backward()
    optimizer.step()
    
    # Calculer accuracy
    _, predicted = torch.max(outputs, 1)
    train_correct += (predicted == y_batch).sum().item()
```

**Phase Validation** :
```python
model.eval()
with torch.no_grad():
    for X_batch, y_batch in valid_loader:
        X_batch, y_batch = X_batch.to(device), y_batch.to(device)
        outputs = model(X_batch)
        _, predicted = torch.max(outputs, 1)
        valid_correct += (predicted == y_batch).sum().item()
```

**Sauvegarder meilleur mod√®le** :
```python
if valid_acc > best_valid_acc:
    best_valid_acc = valid_acc
    torch.save(model.state_dict(), out_dir / "best_model.pth")
```

**√âtape 10 : √âvaluation finale**
- Charger le meilleur mod√®le
- Calculer train_accuracy et valid_accuracy

**√âtape 11 : Cr√©er m√©triques**
```python
metrics = Metrics(
    train_accuracy=train_acc,
    valid_accuracy=valid_acc,
    valid_count=len(valid_items)
)
```

**√âtape 12 : Sauvegarder le bundle**
```python
bundle = PyTorchModelBundle(
    model=model,
    labels=self.labels,
    transformation_engine=self.transformation_engine,
    cfg=model_cfg
)
bundle.save(out_dir / "model")
```

**√âtape 13 : Retourner**
- Retourner `metrics`

---

<a id="predict-pipeline"></a>
## 11. leaffliction/predict_pipeline.py ‚Äî Pipeline de pr√©diction

### Pipeline PyTorch

```
1. Charger le bundle (model.pth, labels.json)
2. Charger et transformer l'image en tensor
3. Pr√©dire avec le mod√®le PyTorch
4. D√©coder le label
5. (Optionnel) Afficher transformations
```

---

### **Classe : PyTorchPredictor**

#### **M√©thode : predict(bundle_zip, image_path, cfg)**

**Ce qu'elle doit faire** :

**Param√®tres** :
- `bundle_zip` : Path vers learnings.zip
- `image_path` : Path vers l'image √† pr√©dire
- `cfg` : PredictConfig

**Retour** :
- `predicted_label` : str (nom de la classe)
- `probs` : Dict[str, float] (probabilit√©s par classe)
- `transformed` : Dict[str, np.ndarray] (transformations pour visualisation)

---

**√âtape 1 : Charger le bundle**
```python
import tempfile

with tempfile.TemporaryDirectory() as temp_dir:
    bundle = self.bundle_loader.load_from_zip(bundle_zip, Path(temp_dir))
```

**√âtape 2 : Charger et transformer l'image**

**Sous-√©tape 2.1 : Charger l'image**
- Utiliser OpenCV : `img = cv2.imread(str(image_path))`
- V√©rifier que l'image est charg√©e (pas None)
- Convertir BGR ‚Üí RGB : `cv2.cvtColor(img, cv2.COLOR_BGR2RGB)`
- Redimensionner : `cv2.resize(img, bundle.cfg.img_size)`

**Sous-√©tape 2.2 : Cr√©er le tensor**
- Appeler `self.transformation_engine.apply_all_as_tensor(img)`
- R√©sultat : tensor de shape `(6, 224, 224)`

**√âtape 3 : Pr√©dire**
```python
pred_id, probs = bundle.predict(tensor)
```

**√âtape 4 : D√©coder le label**
```python
predicted_label = bundle.labels.decode(pred_id)
```

**√âtape 5 : (Optionnel) Appliquer transformations pour visualisation**
```python
transformed = {}
if cfg.show_transforms:
    transformed = self.transformation_engine.apply_all(img)
```

**√âtape 6 : Retourner**
- Retourner `(predicted_label, probs, transformed)`

---

### **Classe : PredictionVisualiser**

#### **M√©thode : show(original, transformed, predicted_label)**

**Ce qu'elle doit faire** :

**Param√®tres** :
- `original` : Image originale (np.ndarray)
- `transformed` : Dict des transformations `{name: img}`
- `predicted_label` : str (classe pr√©dite)

---

**√âtape 1 : Cr√©er le titre**
- `title = f"Prediction: {predicted_label}"`

**√âtape 2 : Utiliser GridPlotter**
```python
from leaffliction.plotting import GridPlotter

grid = GridPlotter()
grid.show_grid(title, transformed, original=original)
```

---

## üìö Ordre d'Impl√©mentation Recommand√©

### **Phase 1 : Dataset (Priorit√© üî¥)**

1. **DatasetScanner.scan()**
   - Lister sous-dossiers
   - Extraire class_names
   - Scanner images
   - Cr√©er DatasetIndex

**Test** : `python Distribution.py ./leaves/images/`

---

### **Phase 2 : Augmentations (Priorit√© üü°)**

2. **Les 6 augmentations**
   - FlipHorizontalAug
   - FlipVerticalAug
   - RotateAug
   - BrightnessContrastAug
   - GaussianBlurAug
   - RandomCropResizeAug

3. **AugmentationEngine.default_six()**
   - Factory pour cr√©er les 6 augmentations

4. **AugmentationEngine.apply_random()**
   - S√©lectionner n augmentations al√©atoires
   - Appliquer s√©quentiellement

5. **AugmentationEngine.augment_dataset()**
   - Cr√©er images physiques sur disque
   - Retourner liste √©tendue

**Test** : `python Augmentation.py ./leaves/images/Apple_healthy/image\ \(1\).JPG`

---

### **Phase 3 : Transformations (Priorit√© üî¥)**

6. **Les 6 transformations**
   - GrayscaleTf
   - CannyEdgesTf
   - HistogramEqualisationTf
   - SharpenTf
   - ThresholdTf
   - MorphologyTf

7. **TransformationEngine.default_six()**
   - Factory pour cr√©er les 6 transformations

8. **TransformationEngine.apply_all()**
   - Pour visualisation
   - Retourne Dict[str, np.ndarray]

9. **TransformationEngine.apply_all_as_tensor()**
   - Cr√©er tensor PyTorch
   - Shape : (6, H, W)

10. **TransformationEngine.batch_transform()**
    - Transformer batch d'images
    - Retourne (X, y) tensors

**Test** : `python Transformation.py ./leaves/images/Apple_healthy/image\ \(1\).JPG`

---

### **Phase 4 : Mod√®le (Priorit√© üî¥)**

11. **LabelEncoder**
    - fit()
    - encode()
    - decode()
    - to_json_dict()
    - from_json_dict()

12. **TransformationClassifier**
    - __init__() : D√©finir architecture
    - forward() : Forward pass

13. **PyTorchModelFactory.build()**
    - Cr√©er TransformationClassifier

14. **PyTorchModelBundle**
    - save() : Sauvegarder model.pth, labels.json
    - load() : Charger depuis dossier
    - load_from_zip() : Charger depuis ZIP
    - predict() : Pr√©dire depuis tensor

**Test** : Cr√©er un petit mod√®le et tester forward pass

---

### **Phase 5 : Training (Priorit√© üî¥)**

15. **PyTorchTrainer.train()**
    - Scanner dataset
    - Split train/valid
    - Augmenter train set
    - Transformer en tensors
    - Cr√©er DataLoaders
    - Training loop
    - √âvaluation
    - Sauvegarder bundle

16. **RequirementsGate.assert_ok()**
    - V√©rifier accuracy > 90%
    - V√©rifier valid_count >= 100

17. **TrainingPackager**
    - prepare_artifacts_dir()
    - build_zip()

**Test** : `python train.py ./leaves/images/ --epochs 5`

---

### **Phase 6 : Pr√©diction (Priorit√© üü¢)**

18. **PyTorchPredictor.predict()**
    - Charger bundle
    - Transformer image
    - Pr√©dire
    - Retourner r√©sultat

19. **PredictionVisualiser.show()**
    - Afficher grille avec transformations

**Test** : `python predict.py learnings.zip ./leaves/images/Apple_healthy/image\ \(1\).JPG`

---

### **Phase 7 : Finalisation (Priorit√© üü¢)**

20. **G√©n√©ration signature.txt**
    - Calculer SHA1 de learnings.zip
    - √âcrire dans signature.txt

21. **Tests end-to-end**
    - Training complet
    - Pr√©diction sur plusieurs images
    - V√©rification accuracy

---

## üéØ Points Cl√©s √† Retenir

### **1. Transformations = Canaux**
- 6 transformations ‚Üí 6 canaux d'entr√©e
- Pas de features manuelles (histogrammes, stats)
- Le CNN apprend directement des transformations

### **2. Augmentations = Images Physiques**
- Cr√©√©es AVANT le training
- Sauvegard√©es sur disque
- Pas d'augmentation √† la vol√©e

### **3. Pipeline PyTorch**
- DataLoaders pour batching
- Training loop avec backpropagation
- Sauvegarde best model

### **4. Architecture Simple**
- 4 Conv2D + GAP + 2 Dense
- ~1M param√®tres
- Entra√Ænement rapide (quelques minutes)

### **5. D√©fendable**
- Architecture claire et interpr√©table
- Transformations explicites
- Performance √©lev√©e (>90%)

---

## üìñ Utilisation de ce Guide

### **Pour l'Impl√©mentation**

1. **Lire la section** correspondant √† la classe
2. **Comprendre les √©tapes** d√©crites
3. **Impl√©menter en Python** en suivant les √©tapes
4. **Tester** avec des donn√©es r√©elles

### **Pour la Soutenance**

1. **Expliquer l'architecture** : Transformations ‚Üí Tensors ‚Üí CNN
2. **Justifier les choix** : Pourquoi 6 transformations ?
3. **D√©fendre la logique** : Pourquoi cette approche ?
4. **R√©pondre aux questions** : Utiliser les explications du guide

---

## üéâ Conclusion

Ce guide explique **litt√©ralement** ce que chaque classe doit faire, **sans code**, pour impl√©menter le projet Leaffliction avec PyTorch.

**Points forts** :
- ‚úÖ Architecture unique et performante
- ‚úÖ Explications d√©taill√©es √©tape par √©tape
- ‚úÖ Ordre d'impl√©mentation recommand√©
- ‚úÖ Tests pour chaque phase

**Prochaine √©tape** : Commencer l'impl√©mentation en suivant l'ordre recommand√© !

**Bon courage ! üöÄ**
